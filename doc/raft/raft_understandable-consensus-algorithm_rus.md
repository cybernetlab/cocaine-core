# Поиск простого и понятного алгоритма конценсусов

Перевод статьи [In Search of an Understandable Consensus Algorithm (Extended Version)](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)

## Diego Ongaro and John Ousterhout. Stanford University

### Общая информация

Raft это алгоритм конценсусов для управлениями жруналами реплик. Работа, выполняемая Raft эквивалентна по объёму и качеству системе (мулти-)Paxos, но его структура отличается от Paxos; за счёт этого Raft более понятен и приспособлен для практического применения. Для обеспечения понимаемости алгоритма Raft разделяет ключевые елементы конценсуса, такие как: выбор лидера, реплика журналов и надёжность, а также усиливает степень связанности для уменьшения количества достигаемых состояний. Результаты учебной демонстрации системы показали, что Raft проще в изучении студентами, чем Paxos. Также для гарантии надёжности Raft содержит новый механизм для изменения участников кластера на основе перекрывающего большинства.

### 1. Вступление

Алгоритмы конценсуса позволяют некоторому количеству машин работать как связанной группе, которая может выдерживать отказы некоторых её членов. Поэтому данные алгоритмы являются ключевыми при построении надёдных программных систем большого масштаба.

В последнее десятилетие доминирующим алгоритмом является Paxos [15, 16]: большинство реализаций алгоритма конценсусов основано на Paxos и обучение студентов алгоритмам конценсуса проводится с использованием Paxos.

К сожалению, Paxos очень сложен для понимания, несмотря на большое количество попыток сделать его более доступным. Более того, его архитектура требует комплексных изменения для практического применения. В результате и архитекторы систем и студенты вынуждены прилагать много усилий при работе с Paxos.

После самостоятельных попыток изучить Paxos, мы решили выработать новый алгоритм конценсуса, который бы предоставлял основу для построения систем и изучения. Наш подход был необычен, поскольку основной целью была _понимаемость_: можем ли мы определить алгоритм конценсуса для практического применения и описать его таким образом, чтобы он был прощу в изучении, чем Paxos? Более того, мы хотели бы, чтобы алгоритм был интуитивен, что немаловажно для системных архитекторов. Было важно, чтобы алгоритм не просто работал, но при это чтобы было ясно почему и как он работает.

Результатом этой работы стал алгоритм консесуса, названный Raft. При проектировании Raft для улучшения понимаемости мы применили такие методики, как декомозиция (Raft разделяет выбор лидера, журнал реплик и надёжность) и уменьшение количества состояний (по сравнению с Paxos, в Raft уменьшена степень неопределённости и набор способов, с помощью которых сервера могут стать несогласующимеся с остальными). Обучение 43-х студентов в различных университетах показало, что Raft значительно более понятен, чем Paxos: после изучения обоих алгоритмов 33 студента отвечали на вопросы о Raft лучше, чем на вопросы о Paxos.

Raft во многом схож с существующими алгоритмами конценсуса (здесь можно упомянуть Oki and Liskov’s Viewstamped Replication [29, 22]), но имеет несколько новых особенностей:

* **Строгий лидер:** Raft использует более строгую форму лидерства, чем другие алгоритмы. К примеру, записи журнала могут передаваться только от лидера к другим серверам. Это упрощает управление журналом реплик и делает Raft проще.
* **Выбор лидера:** Raft использует случайные таймеры для выбора лидеров. Это добавляет некоторые механизмы к уже необходимым в алгоритмах, при этом достигается быстрое и простое разрешение конфликтов.
* **Изменение членства:** механизм Raft для изменения набора серверов в кластере использует новый подход совместного конценсуса, при котором в процессе смены состояния перекрывается большинство двух различных конфигураций. Это позволяет кластеру функционировать в процессе смены конфигураций.

Мы верим, что Raft лучше, чем Paxos или другие алгоритмы конценсуса как для изучения, так и в качестве основы для реализации. Он проще и понятнее других алгоритмов; его описание полностью удовлетворяет все нужды при практических применениях; он имеет несколько open-source реализаций и используется в нескольких компаниях; его надёжность формально описана и доказана; его продуктивность сравнима с другими алгоритмами.

В данном документе описаны: проблема реплицируемых конечных автоматов _(state machines)_ (глава 2), достоинства и недостатки Paxos (глава 3), основной подход к понимаемости алгоритма (глава 4), собсвтенно алгоритм Raft (главы 5&mdash;8), оценка Raft (глава 9), а также, обсуждение различных работ, связанных с Raft (глава 10).

### 2. Реплицируемые конечные автоматы (replicated state machines)

Алгоритмы конценсуса обычно используются в контексте реплицируемых конечных автоматов [37]. При этом подходе, конечные автоматы на наборе серверов выполняют идентичные копии одного состояния и могут продолжать выполнение, даже если один из серверов недоступен. Реплицируемые конечные автоматы используются для того, чтобы решить различные проблемы допустимости отказов в распределённых системах. Для примера, система, в которой присутствует один кластер-лидер, такая как GFS [8], HDFS [38] или RAMCloud [33] обычно использует отедльный реплицируемый конечный автомат для управления избранием лидера и хранения конфигурационной информации чтобы не зависить от отказов лидера. Примерами реплицируемых конечных автоматов являются Chubby [2]
и ZooKeeper [11].

Реплицируемые конечные автоматы обычно описываются с помощью реплицируемого журнала, как показано на рис. 1. Каждый сервер хранит журнал, в котором содержится набор команд, для выполнения конечного автомата на этом сервере. Каждый журнал содержит одинаковые команды в одинаковом порядке. Поэтому каждый конечный автомат выполняет одинаковую последовательность команд. Поскольку конечные автоматы детерминированы, каждое выполнение приводит к одинаковому состоянию и одинаковой последовательности результатов.

Отслеживание целостности реплицируемого журнала это задача алгоритма конценсусов. Модуль конценсусов на сервере получает команды от клиентов и добавляет их в журнал. Затем он устанавливает связь с модулями конценсуса на других серверах, чтобы убедиться, что каждый журнал в конечном итоге содержит одинаковые запросы в одинаковом порядке, даже в случае отказа некоторых серверов. Как только команды полностью реплицированы, каждый конечный автомат на серверах обрабатывает их в указанном порядке и результат возвращается клиенту. В итоге сервера выглядят как единый безотказный конечный автомат.

Алгоритмы конценсуса для практического применения обычно имеют следующие свойства:

* Они дают гарантию надёжности (никогда не возвращается неверный результат) при любых non-Byzantine условиях, включая задержки сети, разделение и потерю пакетов, дублирование и изменение порядка пакетов.
* Они полностью работоспособны (доступны) до тех пор, пока большинство серверов находятся в рабочем состоянии и могут взаимодействовать друг с другом и с клиентами. Таким образом, кластер из пяти серверов способен работать при выключении двух серверов. Сервер считается неработоспособным при его выключении; позже он может быть переведён в рабочее состояние и включён в кластер.
* Целостность журнала не зависит от синхронизации времени. В худшем случае, при очень сильной рассинхронизации времени возможны проблемы с доступностью кластера.
* В общем случае команда может быть выполнена как только большинство в кластере доступно в течении одного раунда удалённых вызовов процедур; меньшинство более медленных серверов не должно влиять на общую производительность системы.

### 3. Что не так с Paxos?

В течении последних десяти лет, протокол Paxos Лесли Лампорта [15] стал синонимом конценсуса: об этом протоколе восновном говорят на курсах и большинство реализаций конценсуса используют его как точку отпарвления. Вначале Paxos определяет протокол для достижения согласия по _единственному решению_, например для одной записи в реплицируемом журнале. Мы называем это _(single-decree Paxos)_. Затем Paxos комбинирует несколько экземпляров данного протокола для принятия серии решений, таких как весь журнал _(multi-Paxos)_. Paxos гарантирует надёжность и живучесть и поддерживает изменения состава кластера. Его корректность была доказана и в обычных случаях это эффективный протокол.

К сожалению, Paxos имеет ряд серьёзных недостатков. Первым из них является очень высокая сложность для понимания. Полное описание [15] известно своей непонятностью; небольшое количество людей приложив массу усилий могут понять о чём там говориться. В результате было сделано несколько попыток описать Paxos в более простых терминах [16, 20, 21]. Эти описания восновном касаются single-decree Paxos и до сих пор в них вносятся изменения. В информационном обзоре участников NSDI 2012 в числе исследователей мы нашли лишь несколько человек, которые хорошо разбирались в Paxos. Мы сами испытывали большие проблемы с Paxos; мы не смогли полностью понять протокол даже после прочтения нескольких упрощённых описаний и разработки нашего альтернативного протокола &mdash; весь этот процесс занял у нас почти год.

Мы предположили, что такая сложность Paxos обусловлена тем, что за его основу взято подмножество single-decree. Paxos single-decree разбит на две стадии, для которых нет простого интуитивного описания и которые невозможно понять по отдельности. Композиционные правила для multi-Paxos добавляют сложности. Мы уверены, что общая проблема при достижении конценсуса для нескольких решений (т.е. всего журнала вместо записей в журнале) может быть разбита на составляющие другими, более непосредственными и понятными способами.

Вторая проблема Paxos в том, что он не предоставляет приемлемой базы для построения практических реализаций. Одна из причин в том, что не существует общепринятого алгоритма для multi-Paxos. Описание Лампорта восновном касается single-decree; он приводит в общих чертах подходы для multi-Paxos, но во много деталей опущено. Было предпринято несколько попыток конкретизировать и оптимизировать Paxos [26, 39, 13], но они отличаются друг от друга и от набросков Лампорта. Такие системы, как Chappy [9] описывают похожие на Paxos алгоритмы, но в основном их детализация не публикуется.

Более того, архитектура Paxos не подходит для построения практических систем; это ещё одно следствие разбиения single-decree. Например, можно получить небольшую выгоду, выбирая набор записей журнала независимо и затем объединяя их в последовательный журнал; но это добовляет сложность системы. Проще и эффективнее разработать для журнала систему, где новые записи добавляются последовательно в принудительном порядке. Другая проблема заключается в том, что Paxos в своём ядре использует симметричный peer-to-peer (хотя в конечном счёте это предполагает оптимизацию быстродействия засчёт слабой формы лидерства). Данный подход актуален в упрощённом мире, где необходимо принять единственное решение, однако его используют несколько практических применений. В случае, когда необходимо принять последовательность решений, проще и быстрее сначала выбрать лидера и затем позволить ему координировать решения.

В результате, практические применения имеют немного сходства с Paxos. Каждая реализация начинается с Paxos, обнаруживает трудности описания и затем разрабатывает полностью отличную архитектуру. Это отнимает много времени и привносит ошибки. Формулмровка Paxos может быть неплоха для доказательства теорем о его корректности, но в реализации настолько отличаются от Paxos, что данные доказательства не имеют особого значения. Приведём типичный комментарий:

> There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.... the final system will be based on an unproven protocol [4].

В свете описанных проблем мы решили, что Paxos не является подходящей базой для построения систем или даже для изучения. При том, что в больших системах алгоритм конценсуса является довольно важным, мы решили постараться разработать альтернативный алгоритм конценсуса, обладающий лучшими чем у Paxos свойствами. Результатом этого эксперимента и является Raft.

### 4. Разработан для ясности

У нас есть несколько целей при разработке Raft: он должен предоставлять полную и практичную базу для построения систем, поэтому значительно сокращено количество проектирования, требуемого от разработчиков; он должен быть надёжным при любых условиях и доступным при обычных условиях работы; также он должен быть эффективным для обычных операций. Но наша первостепенная и самая труднодостижимая цель &mdash; это _яность_ или понятность протокола. Самая широкая аудитория должна без особых проблем понимать алгоритм. Также алгоритм должен быть интуитивен для того, чтобы разработчики систем могли делать расширения, которые неизбежны в различных реализациях.

В разработке Raft было много моментов, когда было необходимо выбрать из нескольких альтернатив один подход. В таких ситуациях мы рассматривали альтернативы с точки зрения ясности: насколько сложно описать каждую альтернативу (например, насколько сложен её набор состояний и имеет ли она скрытый подтекст) и насколько просто читателю будет полность понять данный подход и его реализации?

Мы определили, что при таком анализе мы имеет высокую степень субъективности; тем не менее мы использовали два метода. Первый, это хорошо известный метод декомпозиции: где это возможно, мы разделяли проблемы на отедльные составляющие, которые могли быть решени, описаны и поняты полностью независимо друг от друга. К примеру, в Raft мы разделили выбор лидера, репликацию журналов, надёжность и изменение членства в кластере.

Нашим вторым методом было упрощение набора состояний чтобы сделать систему более понятной и устранить неопределённость везде, где это возможно. К примеру, журналы не могут иметь скважность и Raft ограничивает набор возможных путей, по которым журнал может утратить целостность. Мы старались убрать неопределённость, однако существует несколько ситуаций, когда неопредлённость повышает ясность. В частности, рандомизация привносит неопределённость, но она нужна для уменьшения количества состояний, поскольку обрабатывает любые возможные решения одинаковым способом ("выберай любой &mdash; нам не важно"). Мы используем рандомизацию для упрощения алгоритма выбора лидера.

### 5. Алгоритм конценсуса Raft

Raft это алгоритм для управления реплицируемым журналом в форме, описанной в главе 2. На рис. 2 показан алгоритм в сжатом виде, на рис. 3 приведён список ключевых свойств алгоритма; элементы данных рисунков будет обсуждаться отедльно в этой главе.

Для достижения конценсусв в Raft вначале выбирается _лидер_. Затем лидеру делегируется ответсвенность за реплицирование журнала. Лидер принимает записи журнала от клиентов, реплицирует их на другие сервера, и оповещает сервера о том, когда можно безопасно применить записи журнала к конечным автоматам. Наличие лидера упрощает управление реплицируемым журналом. К примеру, лидер может решить в какое место журнала поместить новые записи без необходимости согласовывать это с другими серверами. При этом потоки данных сводяться к передаче с лидера на другие сервера. Лидер может выйти из строя или отключиться. В этом случае выбирается новый лидер.

При таком подходе Raft разделяет проблему конценсуса на три независимых проблемы, которые будут описаны ниже:

* **выбор лидера:** при отказе существующего лидера должен быть назначен новый лидер (раздел 5.2).
* **репликация журнала:** лидер должен принимать записи журнала от клиентов и реплицировать их в кластер, приводя журналы на всех серверах к данному состоянию (раздел 5.3).
* **надёжность:** ключевое свойство надёжности для Raft это свойство надёжности кончного автомата, изображённое на рис. 3: если какой-либо сервер применил определённую запись журнала к своему конечному автомату, то ни один другой сервер не может применить никакую другую команду для этого же индекса журнала (номера записи). В разделе 5.4 описано, как это реализовано в Raft; решение включает дополнительное ограничение алгоритма выбора лидера, описанное в разделе 5.2

После описания алгоритма конценсуса в данной главе рассматривается проблема доступности и роль синхронизации времени в системе.

#### 5.1. Основы Raft

Кластер Raft содержит некоторое количество серверов; типичное значение &mdash; 5. Данное количество позволяет системе продолжать работу при отказе двух серверов. В любой момент времени каждый сервер находится в одном из трёх состояний: _лидер_, _ведомый_ и _кандидат_. В обычных условиях в системе присутствует только один лидер, все остальные сервера являются ведомыми. Ведомые сервера пассивны: они не генерируют запросов и отвечают на запросы от лидера или кандидата. Лидер обрабатывает все запросы от клиентов (если клиент связывается с ведомым, ведомый перенаправляет его на лидера). Третье состояние, _кандидат_, используется для выбора лидера и описано в разделе 5.2. На рис. 4 показаны все состояния и переходы между ними. Все переходы обсуждаются в данном разделе ниже.

Raft разделяет время на _интервалы_произвольной длины, как это показано на рис. 5. Интервалы аоследовательно пронумерованы с помощью целых чисел. Каждый интервал начинается с _выборов_, в которых один или несколько кандидатов пытаются стать лидерами, как это описано в разделе 5.2. Если кандидат выигрывает виборы, он становится лидером на весь данный верменной интервал. В некоторых случаях выборы дают разделённое голосование. В этом случае интервал заканчивается без лидера и сразу создаётся новый интервал (с новыми выборами). Raft гарантирует, что в одно и тоже время существует только один лидер.

Сервера в кластере отслеживают момент смены интервала и делают это в различные моменты времени, и в некоторых случаях, сервер может пропустить момент выборов или, даже, весь интервал полностью. Интервалы в Raft являются логическими часами [14] и позволяют серверам получать устаревшую информацию, такую как прошлый лидер. Каждый сервер хранит номер _текущего интервала_, который увеличивается со временем. При взаимодействии серверов, происходит обмен текущими интервалами; если у одного из сервером текущий интервал меньше, чем у другого, то сервер с меньшим текущим интервалом увеличивает его до большего зачения. Если кандидат или лидер выисняет, что его текущий интервал просрочен, он тутже становится ведомым. Если сервер получает запрос с просроченным интервалом, данный запрос игнорируется.

Взаимодействие между серверами осуществляется с помощью удалённого вызова процедур (RPC) и для реализации основного алгоритма конценсуса необходимо только два типа RPC. RPC RequestVote, который вызывается кандидатами в момент выборов (раздел 5.2) и RPC AppendEntries, который вызывается лидером для репликации записей журнала (раздел 5.3). В главе 7 добавляется ещё один RPC для передачи снимков между серверами. Если сервер, вызывав RPC, не получил своевременный ответ, запрос повторяется. Для улучшения производительность RPC могут выполнляться на серверах параллельно.

#### 5.2. Выборы лидера

Для инициации выбора лидера в Raft используется механизм heeartbeat. При запуски сервера начинают свою работу, как ведомые. Сервер остаётся в режиме ведомого до тех пор, пока он получает верные RPC запросы от кандидата. Лидер периодически уведомляет ведомых о том, что он в рабочем состоянии (с помощью пустого RPC AppendEntries). Если в течение определённого времени, называемого _таймаут выборов_ ведомый не получает данного подтверждения, то считается, что лидер отсутствует и начинаются выборы нового лидера.

Для начала выборов, ведомый увеличивает текущий интервал и переходит в режим кандидата. Затем он голосует сам за себя и параллельно выполняет RPC запрос RequestVote к каждому серверу в кластере. Кандидат находится в данном состоянии пока непроизойдёт одно из следующих событий, описанных ниже в данной главе:

* он выиграет выборы
* другой сервер назначит себя как лидера
* наступит таймаут и при этом не будет выигравшего

Кандидат выигрывает выборы, если он получает большинство голосов от сервов в данном кластере в конкретный временной интервал. Кажды сервер может проголосовать только за одного кандидата в конкретный интервал времени. Голосование выполняется по принципу "первый пришёл, первый обслужен" (в разделе 5.4 описываются дополнительные ограничения на голосование). Правило большинства гарантирует, что только один кандидат может выиграть выборы в определённый период времени (свойство надёжности выборов на рис. 3). Как только кандидат выигрывает выборы он становится лидером. Далее он посылает hertbeat сообщения всем серверам чтобы подтвердить свои полномочия и предотвратить новые выборы.

Ожидая голоса от серверов кандидат может получить RPC AppendEntries от другого сервера, желающего стать лидером. Если интервал лидера (указанный в RPC) не меньше текущего периода кандидата, то кандидат признаёт лидера и переходит в режим ведомого. Если интервал в RPC меньше текущего интервала кандидата, то он отбрасывает данный запрос и продолжает работать в режиме кандидата.

Третий возможный случай когда кандидат не выигрывает и не проигрывает выборы: если много ведомых становятся кандидатами в одно и тоже время, голоса могут распределиться таким образом, что ни у одного из кандидатов не будет большинства голосов. В этом случае каждый кандидат инициирует новые выборы, увеличивая номер своего интервала и посылая новый набор RPC запросов RequedstVote. Однако без дополнительных действий, разделение голосов может повторяться бесконечно.

Raft использует случайные таймауты выборов для гарантии, что разделение голосов будет происходит редко и данная ситуация будет разрешаться быстро. Для предотвращения разделения голосов сначала используется случайно выбранный из фиксированного интервала (150-300 мс) таймаут выборов. Таким образом сервера разносятся во времени и в большинстве случаев только на одном сервере произойдёт таймаут выборов и он инициирует и выиграет выборы и пошлёт heartbeat пакеты до того, как таймаут выборов наступит на других серверах. Подобный механизм испольлзуется и для разрешения разеделения голосов. Каждый кандидат перезапускает свой случайный счётчик в начале выборов и ждёт в течении этого случайного периода прежде чем начать новые выборы; это снижает верочтность разделения голосов в следующих выборах. В главе 9.3 показано, что с помощью данного подхода лидер выбирается очень быстро.

Выборы это один из примеров того, как ясность или понятность различных подходов влияет на выбор этих подходов при разработке Raft. Изначально мы планировали использовать систему рангов: каждый кандидат получал свой уникальный ранг, который затем использовался для выбора между кандидатами. Если кандидат обнаруживал другого кандидата с большим рагом, он возвращался в режим ведомого, а кандидат с максимальным рангом выигрывал выборы. Однако, мы обнаружили, что данный подход имеет скрытые проблемы с доступностью (сервер с меньшим рангом должен снова стать кандидатом в случае, если сервер с более высоким рангом откажет, однако, если это произойдёт слишком поздно, то весь процесс может сбросться до выбора лидера). Вы делали несколько попыток настроить данный алгоритм, но при каждой настройке возникали новые частные случаи данной ситуации. В конце концов мы решили, что болше подхоит вариант со случайными попытками и что он более понятен.

#### 5.3. Репликация журнала

Как только выбран лидер он начинает обслуживать запросы от клиентов. Кажды запрос от клиента содержит команду, которая будет выполнена с помощью реплицируемых конечных автомтов. Лидер добавляет команду как новую запись в свой журнал и затем параллельно делает RPC запросы AppendEntries ко всем серверам в кластере для того, чтобы реплицировать запись. Когда запись гарантированно реплецирована (как описано ниже), лидер применяет запись к своему конечному автомату и возвращает результат выполнения клиенту. Если видомые вышли из строя или работают медленно или в сети имеются потери пакетов, лидер повторяет AppendEntries RPC вызовы бесконечно (даже после того, как он уже ответил клиенту) до тех пор, пока все ведомые в конечном счёте не сохранят эту запись.

Организация журнала показана на рис. 6. Каждая запись журнала хранит команду конечного автомата и номер интервала, когда запись получена лидером. Номер интервала в записях журнала используется для выявления несоответсвий между журналами и для гарантии некоторых свойств, показаных на рис. 3. Каждая запись журнала также имеет целочисленный индекс, поределяющий её место в журнале.

Лидер решает, когда можно безопасно применить записи журналов к конечным автоматам; такие записи называются _применённые_. Raft гарантирует, что применённые записи надёжны и в конечном счёте будут выполненны на всех доступных конечных автоматах. Запись журнала становится применённой как только лидер, создавший данную запись реплицирует её на большинство серверов (запись №7 на рис. 6). В этом случе также применёнными считаются все записи, предшествующие данной в журнале лидера, включая записи, созданные предыдущими лидерами. В разделе 5.4 осбсуждаются некоторые трудности, связанные с применением этого правила после смены лидера, также там показано, что данное предположение о применённости надёжно. Лидер отслеживает наибольший индекс команды, которая должна быть применена и включает этот индекс во все последующие вызовы AppendEntries (включая вызовы hertbeat). Как только ведомый понимает, что запись применённая, он выполняет её на локальном конечном автомате (в порядке, указанном в журнале).

Журналы Raft разработаны таким образом, чтобы можно было сохранять высокую степень связанности между журналами на разных серверах. Это не только упрощает поведение системы и делает её более предсказуемой, но также является важной частью гарантантирования надёжности. Raft обеспечивает следующие свойства, которые в совокупности дают свойство соответствия журналов на рис. 3:

* если две записи в различных журналах имеют одинаковый индекс и интервал, то в них хранится одна команда.
* если две записи в различных журналах имеют одинаковый индекс и интервал, то все предыдущие записи в журналах идентичны.

Первое утверждение следует из того, что лидер создаёт только одну запись с данным индексом в конкретном интервале и из того, что записи никогда не меняют свою позицию в журнале. Второе свойство обеспечивается простой проверкой целостности, выполняемой в AppendEntries. При вызове AppendEntries, лидер включает индекс и интервал в своём журнале, которые непосредственно предшествуют новым записям. Проверка целостности действует как вводный шаг (induction step): изначально пустой журнал удовлетворят свойству соответствия журналов и проверка целостности сохраняет это свойство при расширении журнала. В результате, если AppendEntires выполнена полностью, лидер знает, что журнал ведомого идентичен его собственному вплоть до последней записи.

При нормальном функционировании журналы лидера и ведомых остаются целостными и проверка целостности в AppendEntries проходит успешно. Однако, сбой лидера может привести к потере целостности журналов (предыдущий лидер может иметь журнал с не полностью реплицированными записями). Данная ситуация может посторяться в результате последовательных отказов лидера и ведомых. На рис. 7 показаны варианты, при которых журналы ведомых могут отличаться от журнала нового лидера. Ведомый может пропустить записи, которые присутствуют у лидера и может иметь дополнительные записи, которых нет на лидере или возможны оба этих случая. Отсутствующие или лишние записи могут появляться в нескольких временных интервалах.

В Raft обработкой таких несоответствий занимается лидер, заставляя журналы ведомых дублировать журнал лидера. Это означает, что конфликтующие записи в журнале ведомого будут перезаписаны записями из журнала лидера. В разделе 5.4 будет показано, что с некоторыми ограничениями такой подход является надёжным.

Для приведения журнала ведомого в соответсвие с журналом лидера, лидер должен найти последнюю запись, до которой журналы совпадают, затем удалить все последующие записи из журнала ведомого и послеать ведомому все записи, начиная с найденной. Все эти действия происходят при запросе проверки целостности, выполняемой в AppendEntries. Лидер хранит индекс следующей записи _nextIndex_ для каждого ведомого. Когда лидер запускается, он инициализирует все значения nextIndex индексом, следующей записи в собственном журнале (11 на рис. 7). Если журнал ведомого не соответствует журналу лидера, проверка целостности AppendEntries завершится неудачно при следующем вызове этой функции. После этого лидер уменьшит nextIndex и повторит запрос AppendEntries. В конце концов nextIndex достигнет заприси одинаковой на лидере и на ведомом. После этого AppendEntries будет выполнена успешно, в следствии чего на ведомом удалятся все конфликтующие записи и добавяться все записи с лидера (если таковые имеются). Как только AppendEntries выполнена успешно, журналы лидера и ведомого совпадают и данное утверждение останется верным до конца периода.

Если это необходимо, протокол может быть оптимизирован путём уменьшения неудачных вызовов AppendEntries. Для примера, при неудачном запросе AppendEntires ведомый может включить в ответ номер интервала для конфликтующей записи и первый индекс для данного интервала. Обладая этой информацией лидер может уменьшать nextIndex не на единицу, а до первого индекса в интервале; таким образом будет необходим один вызов AppendEntries на интервал с конфликтующими записями вместо одного вызова на запись. Мы смоневаемся, что данная оптимизация необходима на практике, поскольку такие несоответствия записей происходят не часто и при этом появляется небольшое число конфликтующих записей.

Используя данный механизм лидер избавляется от необходимости иметь специальную логику для восстановления целостности журналов при старте. Он просто начинает своё обычное функционирование и журналы автоматически приводятся в одинаковое состояние с помощью проверки целостности в AppendEntries. Лидер никогда не перезаписывает и не удаляет записи в своём журнале (свойство только добавления в журнал лидера на рис. 3).

Данный способ репликации журналов предоставляет необходимые свойства конценсуса, описанные в главе 2: Raft может принимать, реплицировать и применять новые записи журнала до тех пор, пока большинство серверов находится в рабочем состоянии; в обычном случае одна запись может быть реплицирована за один раунд RPC запросов к большинству серверов в кластере; один медленный ведомые не влияет на общую производительность системы.

#### 5.4. Надёжность

В предыдущих разделах показано, как Raft выбирает лидера и реплицирует записи журналов. Однако, описанных механизмов недостаточно для гарантии того, что каждый конечный автомат выполнит абсолютно одинаковую последовательность команд в одинаковом порядке. Для примера, ведомый может быть не доступен в тот момент, когда лидер отправляет некоторые записи журнала, а затем он (ведомый) может быть выбран как лидер и перезапишет данные записи другими; в результате различные конечные автоматы могут выполнить различные последовательности команд.

Данная глава завершает описание алгоритма Raft добавляя ограничивающие условия, при которых сервер может стать лидером. Эти ограничения гарантируют, что лидер для любого временного интервала содержит все применённые записи из предыдущих интервалов (свойство полноты лидера на рис. 3). Имея ограничение на избрание мы затем уточним правила применения записей. В конце мы покажем пример свойства полноты лидера и покажем, как оно корректирует поведение реплицируемых конечных автоматов.

##### 5.4.1. Ограничение избрания

В любом алгоритме конценсуса, основанном на лидере, лидер должен в конечном счёте хранить все применённые записи журнала. В некоторых алгоритмах конценсуса, таких как Viewstamped Replication [22] лидер может быть избран даже если он изначально не содержит всех применённых записей. Такие алгоритмы содержат дополнительную логику для поиска отутствующих записей и передачи их новому лидеру непосредственно в момент избрания или сразу после этого. К сожалению это увеличивает сложность системы. Raft использует более простой подход, в котором гарантируется, что все применённые записи в предыдущих интервалах присутствуют на любом новом лидере с момента его избрания без необходимости передавать такие записи лидеру. Это означает, что записи журнала передаются только в одном направлении &mdash; от лидера к ведомым и лидер никогда не перезаписывает записи в журнале.

Raft использует процесс голосования чтобы недопустить выбор кандидата лидером до тех пор, пока его журнал не будет содержать все применённые записи. Чтобы быть избраным, кандидат должен получить большинство голосов в кластере, что означает, что каждая применённая запись должна существовать по крайней мере на одном из серверов, отдавших свой голос за кандидата. Если журнал кандидата такой же или более свежий в сравнении с любым жруналом из большинства (понятие "более свежий" будет рассмотрено ниже), то это означает, что он хранит все применённые записи. Данное ограничение описывается в RPC запросе RequestVote: при вызове передаётся информация о журнале кандидата и голосующий не отдаёт свой голос за кандидата, если его журнал свежее журнала кандидата.

Raft определяет, какой журнал более свежий сравнивая индекс и номер интервала последней записи в журнале. Если два журнала имеют различные интервалы, то журнал с последним интервалом считается более свежим. Если интервалы совпадают, журнал с большим количеством записей считается более свежим.

##### 5.4.2. Применение записей из предыдущих интервалов

Как описано в разделе 5.3 лидер знает, что однажды применённая запись в текущем интервале сохранена на большинстве серверов. Если на лидере произойдёт сбой до того, как записи будут применены, будущие лидеру будут пытаться закончить репликацию данной записи. Однако, лидер не может сразу решить, является ли запись из предыдущего интервала применённой если она сохранена на большинстве серверов. На рис. 8 приведена ситуация, когда старая запись журнала сохранённая на большинстве серверов может быть перезаписана следующим лидером.

Для исключения подобных проблем в Raft никогда не применяются записи журналов из предыдущих интервалов при помощь подсчёта реплик. Только записи журнала к текущем интервале лидера могут быть применены методом подсчёта реплик; как только запись из текущего интервала применена таким образом, все предыдущие записи применяюстя косвенно исходя из свойства соответствия журналов. Существуют ситуации, когда лидер может безопасно предположить, что старая запись журнала применена (для примера, если запись сохранена на каждом сервере), но Raft использует более консервативный подход для простоты.

Raft исключает данное усложнение в правилах применения, поскольку записи журнала хранят изначальные номера интервалов на момент, когда лидер реплицирует записи из предыдущих интервалов. В других алгоритмах конценсуса если новый лидер реплицирует записи из предыдущих интервалов, он должен делать это с новым номером интервала. Raft облегчает этот механизм, поскольку записи журнала хранят тотже одинаковый номер интервала независимо от времени и конкретного журнала. В дополнении в Raft новые лидеры шлют меньшее количество записей из предыдущего интервала, чем в других алгоритмах (другие алгоритмы вынуждены слать лишние записи чтобы перенумеровать их перед применением).

##### 5.4.3. Аргументация надёжности

Имея полный алгоритм Raft, мы теперь можем точнее говорить о том, присутствует ли свойство полноты лидера (данный аргумент основывается на докозательстве надёжности; см. главу 9.2). Мы предполагаем, что свойство полноты лидера не присутствует если мы докажем обратное. Допустим, лидер для интервала T (leader-T) применяет запись журнала в данном интервале, но эта запись не сохраняется лидером некоторого будущего интервала. Предположим мы имеем, малый интервал U, при этом U > T (интервал U следует за интервалом T), лидер которого (leader-U) не хранит эту запись.

1. Применённая запись должна отсутствовать в журнале лидера U на момент его избрания (лидеры никогда не перезаписывают и не удаляют записи).
2. Лидер T реплицировал запись на большинство серверов кластера. Таким образом, по карйней мере один сервер (голосующий) и принял запись от лидера T и голосует за лидера U, как показано на рис. 9. Голосующий в данном случае является ключевым для достижения противоречия.
3. Голосующий должен иметь применённую запись от лидера T _перед_ тем, как он будет голосовать за лидера U; иначе он отклонит запрос AppendEntries от лидера T (его текущий интервал будет выше, чем интервал T).
4. Голосующий сохраняет запись на момент голосования за лидера U, поскольку любой предполагаемый лидер содержит запись (по определению), лидеры никогда не удаляют записи и ведомые удаляют записи только в случае конфикта с лидерами.
5. Голосующий предоставляет свой голос лидеру U в случае, если журнал лидера U такой же или более свежий, чем журнал голосующего. Из этого следует одно или два противоречия.
6. Во-первых, если у голосующего и лидера U одинаковый номер интервала, то журнал лидера U должен быть той же длины, что и журнал голосующего. Однако это невозможно, поскольку журнал голосующего содержит применённую запись, а журнал лидера U нет.
7. В другом случае, интервал последней записи в журнал у лидера U должен быть больше, чем интервал голосующего. Более того, он должен быть больше чем T, поскольку интервал последней записи в журнал у голосующего был как минимум T (он содержит применённую запись из интервала T). Предыдущий лидер, который создал последнюю запись в журнале лидера U должен был иметь эту применённую запись в своём журнале (по определению). Теперь, следуя свойству соответсвия журналов, журнал лидера U должен также содержать применённую запись, что и является противоречием.
8. Таким образом мы описали полное противоречие. Т.е. все лидеры для интервалов после T должны содержать все записи из интервала T, которые были применены в интервале T.
9. Свойство соответствия журналов гарантирует что будущие лидеры также будут содержать записи, которые были применены косвено, такие как с индексом 2 на рис. 8 (d).

Имея свойство применённого лидера мы можем доказать свойство надёжности конечных автоматов (рис. 3), которое указывает, что если сервер применил запись журнала с определённым индексом, то ни один другой сервер никогда не применит другую запись журнала с тем же индексом. На момент, когда сервер применяет запись журнала к своему конечному автомату, его журнал должен быть идентичен журналу лидера вплоть до данной записи и данная запись должна быть применённой. Теперь предоложим, что имеется некий предыдущий интервал, в котором некий сервер применил данный индекс; свойство соответствия журналов гарантирует, что лидеры для всех последующих интервалов будут хранить эту одинаковую запись журнала и сервера, которые будут применять этот индекс в будущих интервалах применят одинаковое значение. Что означает, что свойство надёжности конечных автоматов присутствует.

В заключении, Raft требует от серверов применять записи в порядке индекса. Принимая во внимание свойство надёжности конечных автомтов, это означает, что все сервера применят абсолютно одинаковую последовательность записей журнала к своим конечным автоматам в одинаковом порядке.

#### 5.5. Отказы ведомых и кандидатов

До данного момента мы рассматривали отказы лидеров. Отказы ведомых и кандидатов обрабатывать проще, чем отказы лидеров и оба этих отакза обрабатываются одинаковым образом. Если произошёл отказ ведомого или кандидата, то следующие запросы RequestVote и AppendEntries закончатся неудачей. В Raft такие неудачные запросы повторяются бесконечно; если отказавший сервер перезапущен, то все запросы RPC будут успешно завершены. Если сервер отказал до завершения вызова но после того, как он уже ответил на запрос, то он получит этот же запрос после перезапуска. RPC в Raft иденпотентны (idempotent), поэтому такие повторения не являются проблемой. Для примера, если ведомый получит запрос AppendEntries, который содержит записи журнала, которые уже существуют в журнале, то он просто проигнорирует такие записи в следующих запросах.

#### 5.6. Синхронизация и доступность

Одно из условий Raft заключается в том, что надёжность не должна зависеть от инхронизации: система не должна возвращать неправильный результат просто из-за того, что события происходят быстрее или медленнее, чем это может ожидаться. Однако, доступность (возможность системы своевеременно отвечать клиентам) неизбежно будет зависеть от синхронизации. Для примера, если обмен сообщениями запнимает больше времени чем обычно требуется для перезапуска после отказа сервера, кандидаты не смогут ожидать достаточно времени для победы в выборах; без существующего лидера Raft не сможет функционировать.

Выбор лидера, это та часть Raft, где синхронизация наиболее сильно влияет на процесс. Raft способен выбрать и сохранять лидера настолько долго, пока система удовлетворяет следующему _условию синхронизации_:

broadcastTime << electionTimeout << MTBF

в данном уравнении _broadcastTime_ это среднее время, затрачиваемое сервером на параллельную посылку RPC запроса каждому серверу в кластере и получение ответов от них; _electionTimeout_ это таймаут выборов, описанный в разделе 5.2; и _MBTF_ это среднее время между отказами для одного сервера