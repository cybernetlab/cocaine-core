# Поиск простого и понятного алгоритма конценсусов

Перевод статьи [In Search of an Understandable Consensus Algorithm (Extended Version)](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)

## Diego Ongaro and John Ousterhout. Stanford University

### Общая информация

Raft это алгоритм конценсусов для управлениями жруналами реплик. Работа, выполняемая Raft эквивалентна по объёму и качеству системе (мулти-)Paxos, но его структура отличается от Paxos; за счёт этого Raft более понятен и приспособлен для практического применения. Для обеспечения понимаемости алгоритма Raft разделяет ключевые елементы конценсуса, такие как: выбор лидера, реплика журналов и надёжность, а также усиливает степень связанности для уменьшения количества достигаемых состояний. Результаты учебной демонстрации системы показали, что Raft проще в изучении студентами, чем Paxos. Также для гарантии надёжности Raft содержит новый механизм для изменения участников кластера на основе перекрывающего большинства.

### 1. Вступление

Алгоритмы конценсуса позволяют некоторому количеству машин работать как связанной группе, которая может выдерживать отказы некоторых её членов. Поэтому данные алгоритмы являются ключевыми при построении надёдных программных систем большого масштаба.

В последнее десятилетие доминирующим алгоритмом является Paxos [15, 16]: большинство реализаций алгоритма конценсусов основано на Paxos и обучение студентов алгоритмам конценсуса проводится с использованием Paxos.

К сожалению, Paxos очень сложен для понимания, несмотря на большое количество попыток сделать его более доступным. Более того, его архитектура требует комплексных изменения для практического применения. В результате и архитекторы систем и студенты вынуждены прилагать много усилий при работе с Paxos.

После самостоятельных попыток изучить Paxos, мы решили выработать новый алгоритм конценсуса, который бы предоставлял основу для построения систем и изучения. Наш подход был необычен, поскольку основной целью была _понимаемость_: можем ли мы определить алгоритм конценсуса для практического применения и описать его таким образом, чтобы он был прощу в изучении, чем Paxos? Более того, мы хотели бы, чтобы алгоритм был интуитивен, что немаловажно для системных архитекторов. Было важно, чтобы алгоритм не просто работал, но при это чтобы было ясно почему и как он работает.

Результатом этой работы стал алгоритм консесуса, названный Raft. При проектировании Raft для улучшения понимаемости мы применили такие методики, как декомозиция (Raft разделяет выбор лидера, журнал реплик и надёжность) и уменьшение количества состояний (по сравнению с Paxos, в Raft уменьшена степень неопределённости и набор способов, с помощью которых сервера могут стать несогласующимеся с остальными). Обучение 43-х студентов в различных университетах показало, что Raft значительно более понятен, чем Paxos: после изучения обоих алгоритмов 33 студента отвечали на вопросы о Raft лучше, чем на вопросы о Paxos.

Raft во многом схож с существующими алгоритмами конценсуса (здесь можно упомянуть Oki and Liskov’s Viewstamped Replication [29, 22]), но имеет несколько новых особенностей:

* **Строгий лидер:** Raft использует более строгую форму лидерства, чем другие алгоритмы. К примеру, записи журнала могут передаваться только от лидера к другим серверам. Это упрощает управление журналом реплик и делает Raft проще.
* **Выбор лидера:** Raft использует случайные таймеры для выбора лидеров. Это добавляет некоторые механизмы к уже необходимым в алгоритмах, при этом достигается быстрое и простое разрешение конфликтов.
* **Изменение членства:** механизм Raft для изменения набора серверов в кластере использует новый подход совместного конценсуса, при котором в процессе смены состояния перекрывается большинство двух различных конфигураций. Это позволяет кластеру функционировать в процессе смены конфигураций.

Мы верим, что Raft лучше, чем Paxos или другие алгоритмы конценсуса как для изучения, так и в качестве основы для реализации. Он проще и понятнее других алгоритмов; его описание полностью удовлетворяет все нужды при практических применениях; он имеет несколько open-source реализаций и используется в нескольких компаниях; его надёжность формально описана и доказана; его продуктивность сравнима с другими алгоритмами.

В данном документе описаны: проблема реплицируемых конечных автоматов _(state machines)_ (глава 2), достоинства и недостатки Paxos (глава 3), основной подход к понимаемости алгоритма (глава 4), собсвтенно алгоритм Raft (главы 5&mdash;8), оценка Raft (глава 9), а также, обсуждение различных работ, связанных с Raft (глава 10).

### 2. Реплицируемые конечные автоматы (replicated state machines)

Алгоритмы конценсуса обычно используются в контексте реплицируемых конечных автоматов [37]. При этом подходе, конечные автоматы на наборе серверов выполняют идентичные копии одного состояния и могут продолжать выполнение, даже если один из серверов недоступен. Реплицируемые конечные автоматы используются для того, чтобы решить различные проблемы допустимости отказов в распределённых системах. Для примера, система, в которой присутствует один кластер-лидер, такая как GFS [8], HDFS [38] или RAMCloud [33] обычно использует отедльный реплицируемый конечный автомат для управления избранием лидера и хранения конфигурационной информации чтобы не зависить от отказов лидера. Примерами реплицируемых конечных автоматов являются Chubby [2]
и ZooKeeper [11].

Реплицируемые конечные автоматы обычно описываются с помощью реплицируемого журнала, как показано на рис. 1. Каждый сервер хранит журнал, в котором содержится набор команд, для выполнения конечного автомата на этом сервере. Каждый журнал содержит одинаковые команды в одинаковом порядке. Поэтому каждый конечный автомат выполняет одинаковую последовательность команд. Поскольку конечные автоматы детерминированы, каждое выполнение приводит к одинаковому состоянию и одинаковой последовательности результатов.

Отслеживание целостности реплицируемого журнала это задача алгоритма конценсусов. Модуль конценсусов на сервере получает команды от клиентов и добавляет их в журнал. Затем он устанавливает связь с модулями конценсуса на других серверах, чтобы убедиться, что каждый журнал в конечном итоге содержит одинаковые запросы в одинаковом порядке, даже в случае отказа некоторых серверов. Как только команды полностью реплицированы, каждый конечный автомат на серверах обрабатывает их в указанном порядке и результат возвращается клиенту. В итоге сервера выглядят как единый безотказный конечный автомат.

Алгоритмы конценсуса для практического применения обычно имеют следующие свойства:

* Они дают гарантию надёжности (никогда не возвращается неверный результат) при любых non-Byzantine условиях, включая задержки сети, разделение и потерю пакетов, дублирование и изменение порядка пакетов.
* Они полностью работоспособны (доступны) до тех пор, пока большинство серверов находятся в рабочем состоянии и могут взаимодействовать друг с другом и с клиентами. Таким образом, кластер из пяти серверов способен работать при выключении двух серверов. Сервер считается неработоспособным при его выключении; позже он может быть переведён в рабочее состояние и включён в кластер.
* Целостность журнала не зависит от синхронизации времени. В худшем случае, при очень сильной рассинхронизации времени возможны проблемы с доступностью кластера.
* В общем случае команда может быть выполнена как только большинство в кластере доступно в течении одного раунда удалённых вызовов процедур; меньшинство более медленных серверов не должно влиять на общую производительность системы.

### 3. Что не так с Paxos?

В течении последних десяти лет, протокол Paxos Лесли Лампорта [15] стал синонимом конценсуса: об этом протоколе восновном говорят на курсах и большинство реализаций конценсуса используют его как точку отпарвления. Вначале Paxos определяет протокол для достижения согласия по _единственному решению_, например для одной записи в реплицируемом журнале. Мы называем это _(single-decree Paxos)_. Затем Paxos комбинирует несколько экземпляров данного протокола для принятия серии решений, таких как весь журнал _(multi-Paxos)_. Paxos гарантирует надёжность и живучесть и поддерживает изменения состава кластера. Его корректность была доказана и в обычных случаях это эффективный протокол.

К сожалению, Paxos имеет ряд серьёзных недостатков. Первым из них является очень высокая сложность для понимания. Полное описание [15] известно своей непонятностью; небольшое количество людей приложив массу усилий могут понять о чём там говориться. В результате было сделано несколько попыток описать Paxos в более простых терминах [16, 20, 21]. Эти описания восновном касаются single-decree Paxos и до сих пор в них вносятся изменения. В информационном обзоре участников NSDI 2012 в числе исследователей мы нашли лишь несколько человек, которые хорошо разбирались в Paxos. Мы сами испытывали большие проблемы с Paxos; мы не смогли полностью понять протокол даже после прочтения нескольких упрощённых описаний и разработки нашего альтернативного протокола &mdash; весь этот процесс занял у нас почти год.

Мы предположили, что такая сложность Paxos обусловлена тем, что за его основу взято подмножество single-decree. Paxos single-decree разбит на две стадии, для которых нет простого интуитивного описания и которые невозможно понять по отдельности. Композиционные правила для multi-Paxos добавляют сложности. Мы уверены, что общая проблема при достижении конценсуса для нескольких решений (т.е. всего журнала вместо записей в журнале) может быть разбита на составляющие другими, более непосредственными и понятными способами.

Вторая проблема Paxos в том, что он не предоставляет приемлемой базы для построения практических реализаций. Одна из причин в том, что не существует общепринятого алгоритма для multi-Paxos. Описание Лампорта восновном касается single-decree; он приводит в общих чертах подходы для multi-Paxos, но во много деталей опущено. Было предпринято несколько попыток конкретизировать и оптимизировать Paxos [26, 39, 13], но они отличаются друг от друга и от набросков Лампорта. Такие системы, как Chappy [9] описывают похожие на Paxos алгоритмы, но в основном их детализация не публикуется.

Более того, архитектура Paxos не подходит для построения практических систем; это ещё одно следствие разбиения single-decree. Например, можно получить небольшую выгоду, выбирая набор записей журнала независимо и затем объединяя их в последовательный журнал; но это добовляет сложность системы. Проще и эффективнее разработать для журнала систему, где новые записи добавляются последовательно в принудительном порядке. Другая проблема заключается в том, что Paxos в своём ядре использует симметричный peer-to-peer (хотя в конечном счёте это предполагает оптимизацию быстродействия засчёт слабой формы лидерства). Данный подход актуален в упрощённом мире, где необходимо принять единственное решение, однако его используют несколько практических применений. В случае, когда необходимо принять последовательность решений, проще и быстрее сначала выбрать лидера и затем позволить ему координировать решения.

В результате, практические применения имеют немного сходства с Paxos. Каждая реализация начинается с Paxos, обнаруживает трудности описания и затем разрабатывает полностью отличную архитектуру. Это отнимает много времени и привносит ошибки. Формулмровка Paxos может быть неплоха для доказательства теорем о его корректности, но в реализации настолько отличаются от Paxos, что данные доказательства не имеют особого значения. Приведём типичный комментарий:

> There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.... the final system will be based on an unproven protocol [4].

В свете описанных проблем мы решили, что Paxos не является подходящей базой для построения систем или даже для изучения. При том, что в больших системах алгоритм конценсуса является довольно важным, мы решили постараться разработать альтернативный алгоритм конценсуса, обладающий лучшими чем у Paxos свойствами. Результатом этого эксперимента и является Raft.

### 4. Разработан для ясности

У нас есть несколько целей при разработке Raft: он должен предоставлять полную и практичную базу для построения систем, поэтому значительно сокращено количество проектирования, требуемого от разработчиков; он должен быть надёжным при любых условиях и доступным при обычных условиях работы; также он должен быть эффективным для обычных операций. Но наша первостепенная и самая труднодостижимая цель &mdash; это _яность_ или понятность протокола. Самая широкая аудитория должна без особых проблем понимать алгоритм. Также алгоритм должен быть интуитивен для того, чтобы разработчики систем могли делать расширения, которые неизбежны в различных реализациях.

В разработке Raft было много моментов, когда было необходимо выбрать из нескольких альтернатив один подход. В таких ситуациях мы рассматривали альтернативы с точки зрения ясности: насколько сложно описать каждую альтернативу (например, насколько сложен её набор состояний и имеет ли она скрытый подтекст) и насколько просто читателю будет полность понять данный подход и его реализации?

Мы определили, что при таком анализе мы имеет высокую степень субъективности; тем не менее мы использовали два метода. Первый, это хорошо известный метод декомпозиции: где это возможно, мы разделяли проблемы на отедльные составляющие, которые могли быть решени, описаны и поняты полностью независимо друг от друга. К примеру, в Raft мы разделили выбор лидера, репликацию журналов, надёжность и изменение членства в кластере.

Нашим вторым методом было упрощение набора состояний чтобы сделать систему более понятной и устранить неопределённость везде, где это возможно. К примеру, журналы не могут иметь скважность и Raft ограничивает набор возможных путей, по которым журнал может утратить целостность. Мы старались убрать неопределённость, однако существует несколько ситуаций, когда неопредлённость повышает ясность. В частности, рандомизация привносит неопределённость, но она нужна для уменьшения количества состояний, поскольку обрабатывает любые возможные решения одинаковым способом ("выберай любой &mdsh; нам не важно"). Мы используем рандомизацию для упрощения алгоритма выбора лидера.

### 5. Алгоритм конценсуса Raft

Raft это алгоритм для управления реплицируемым журналом в форме, описанной в главе 2. На рис. 2 показан алгоритм в сжатом виде, на рис. 3 приведён список ключевых свойств алгоритма; элементы данных рисунков будет обсуждаться отедльно в этой главе.

Для достижения конценсусв в Raft вначале выбирается _лидер_. Затем лидеру делегируется ответсвенность за реплицирование журнала. Лидер принимает записи журнала от клиентов, реплицирует их на другие сервера, и оповещает сервера о том, когда можно безопасно применить записи журнала к конечным автоматам. Наличие лидера упрощает управление реплицируемым журналом. К примеру, лидер может решить в какое место журнала поместить новые записи без необходимости согласовывать это с другими серверами. При этом потоки данных сводяться к передаче с лидера на другие сервера. Лидер может выйти из строя или отключиться. В этом случае выбирается новый лидер.

При таком подходе Raft разделяет проблему конценсуса на три независимых проблемы, которые будут описаны ниже:

* **выбор лидера:** при отказе существующего лидера должен быть назначен новый лидер (раздел 5.2).
* **репликация журнала:** лидер должен принимать записи журнала от клиентов и реплицировать их в кластер, приводя журналы на всех серверах к данному состоянию (раздел 5.3).
* **надёжность:** ключевое свойство надёжности для Raft это свойство надёжности кончного автомата, изображённое на рис. 3: если какой-либо сервер применил определённую запись журнала к своему конечному автомату, то ни один другой сервер не может применить никакую другую команду для этого же индекса журнала (номера записи). В разделе 5.4 описано, как это реализовано в Raft; решение включает дополнительное ограничение алгоритма выбора лидера, описанное в разделе 5.2

После описания алгоритма конценсуса в данной главе рассматривается проблема доступности и роль синхронизации времени в системе.

#### 5.1. Основы Raft

Кластер Raft содержит некоторое количество серверов; типичное значение &mdash; 5. Данное количество позволяет системе продолжать работу при отказе двух серверов. В любой момент времени каждый сервер находится в одном из трёх состояний: _лидер_, _ведомый_ и _кандидат_. В обычных условиях в системе присутствует только один лидер, все остальные сервера являются ведомыми. Ведомые сервера пассивны: они не генерируют запросов и отвечают на запросы от лидера или кандидата. Лидер обрабатывает все запросы от клиентов (если клиент связывается с ведомым, ведомый перенаправляет его на лидера). Третье состояние, _кандидат_, используется для выбора лидера и описано в разделе 5.2. На рис. 4 показаны все состояния и переходы между ними. Все переходы обсуждаются в данном разделе ниже.

Raft разделяет время на _интервалы_произвольной длины, как это показано на рис. 5. Интервалы аоследовательно пронумерованы с помощью целых чисел. Каждый интервал начинается с _выборов_, в которых один или несколько кандидатов пытаются стать лидерами, как это описано в разделе 5.2. Если кандидат выигрывает виборы, он становится лидером на весь данный верменной интервал. В некоторых случаях выборы дают разделённое голосование. В этом случае интервал заканчивается без лидера и сразу создаётся новый интервал (с новыми выборами). Raft гарантирует, что в одно и тоже время существует только один лидер.

Сервера в кластере отслеживают момент смены интервала и делают это в различные моменты времени, и в некоторых случаях, сервер может пропустить момент выборов или, даже, весь интервал полностью. Интервалы в Raft являются логическими часами [14] и позволяют серверам получать устаревшую информацию, такую как прошлый лидер. Каждый сервер хранит номер _текущего интервала_, который увеличивается со временем. При взаимодействии серверов, происходит обмен текущими интервалами; если у одного из сервером текущий интервал меньше, чем у другого, то сервер с меньшим текущим интервалом увеличивает его до большего зачения. Если кандидат или лидер выисняет, что его текущий интервал просрочен, он тутже становится ведомым. Если сервер получает запрос с просроченным интервалом, данный запрос игнорируется.

Взаимодействие между серверами осуществляется с помощью удалённого вызова процедур (RPC) и для реализации основного алгоритма конценсуса необходимо только два типа RPC. RPC RequestVote, который вызывается кандидатами в момент выборов (раздел 5.2) и RPC AppendEntries, который вызывается лидером для репликации записей журнала (раздел 5.3). В главе 7 добавляется ещё один RPC для передачи снимков между серверами. Если сервер, вызывав RPC, не получил своевременный ответ, запрос повторяется. Для улучшения производительность RPC могут выполнляться на серверах параллельно.

#### 5.2. Выборы лидера

