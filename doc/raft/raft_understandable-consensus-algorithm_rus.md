# Поиск простого и понятного алгоритма конценсусов

Перевод статьи [In Search of an Understandable Consensus Algorithm (Extended Version)](https://ramcloud.stanford.edu/wiki/download/attachments/11370504/raft.pdf)

**Авторы: Diego Ongaro и John Ousterhout. Stanford University**

## Общая информация

Raft это алгоритм конценсусов для управлениями жруналами реплик. Работа, выполняемая Raft эквивалентна по объёму и качеству системе (мулти-)Paxos, но его структура отличается от Paxos; за счёт этого Raft более понятен и приспособлен для практического применения. Для обеспечения понимаемости алгоритма Raft разделяет ключевые елементы конценсуса, такие как: выбор лидера, реплика журналов и надёжность, а также усиливает степень связанности для уменьшения количества достигаемых состояний. Результаты учебной демонстрации системы показали, что Raft проще в изучении студентами, чем Paxos. Также для гарантии надёжности Raft содержит новый механизм для изменения участников кластера на основе перекрывающего большинства.

## 1. Вступление

Алгоритмы конценсуса позволяют некоторому количеству машин работать как связанной группе, которая может выдерживать отказы некоторых её членов. Поэтому данные алгоритмы являются ключевыми при построении надёдных программных систем большого масштаба.

В последнее десятилетие доминирующим алгоритмом является Paxos [15, 16]: большинство реализаций алгоритма конценсусов основано на Paxos и обучение студентов алгоритмам конценсуса проводится с использованием Paxos.

К сожалению, Paxos очень сложен для понимания, несмотря на большое количество попыток сделать его более доступным. Более того, его архитектура требует комплексных изменения для практического применения. В результате и архитекторы систем и студенты вынуждены прилагать много усилий при работе с Paxos.

После самостоятельных попыток изучить Paxos, мы решили выработать новый алгоритм конценсуса, который бы предоставлял основу для построения систем и изучения. Наш подход был необычен, поскольку основной целью была _понимаемость_: можем ли мы определить алгоритм конценсуса для практического применения и описать его таким образом, чтобы он был прощу в изучении, чем Paxos? Более того, мы хотели бы, чтобы алгоритм был интуитивен, что немаловажно для системных архитекторов. Было важно, чтобы алгоритм не просто работал, но при это чтобы было ясно почему и как он работает.

Результатом этой работы стал алгоритм консесуса, названный Raft. При проектировании Raft для улучшения понимаемости мы применили такие методики, как декомозиция (Raft разделяет выбор лидера, журнал реплик и надёжность) и уменьшение количества состояний (по сравнению с Paxos, в Raft уменьшена степень неопределённости и набор способов, с помощью которых сервера могут стать несогласующимеся с остальными). Обучение 43-х студентов в различных университетах показало, что Raft значительно более понятен, чем Paxos: после изучения обоих алгоритмов 33 студента отвечали на вопросы о Raft лучше, чем на вопросы о Paxos.

Raft во многом схож с существующими алгоритмами конценсуса (здесь можно упомянуть Oki and Liskov’s Viewstamped Replication [29, 22]), но имеет несколько новых особенностей:

* **Строгий лидер:** Raft использует более строгую форму лидерства, чем другие алгоритмы. К примеру, записи журнала могут передаваться только от лидера к другим серверам. Это упрощает управление журналом реплик и делает Raft проще.
* **Выбор лидера:** Raft использует случайные таймеры для выбора лидеров. Это добавляет некоторые механизмы к уже необходимым в алгоритмах, при этом достигается быстрое и простое разрешение конфликтов.
* **Изменение членства:** механизм Raft для изменения набора серверов в кластере использует новый подход совместного конценсуса, при котором в процессе смены состояния перекрывается большинство двух различных конфигураций. Это позволяет кластеру функционировать в процессе смены конфигураций.

Мы верим, что Raft лучше, чем Paxos или другие алгоритмы конценсуса как для изучения, так и в качестве основы для реализации. Он проще и понятнее других алгоритмов; его описание полностью удовлетворяет все нужды при практических применениях; он имеет несколько open-source реализаций и используется в нескольких компаниях; его надёжность формально описана и доказана; его продуктивность сравнима с другими алгоритмами.

В данном документе описаны: проблема реплицируемых конечных автоматов _(state machines)_ (глава 2), достоинства и недостатки Paxos (глава 3), основной подход к понимаемости алгоритма (глава 4), собсвтенно алгоритм Raft (главы 5&mdash;8), оценка Raft (глава 9), а также, обсуждение различных работ, связанных с Raft (глава 10).

## 2. Реплицируемые конечные автоматы (replicated state machines)

Алгоритмы конценсуса обычно используются в контексте реплицируемых конечных автоматов [37]. При этом подходе, конечные автоматы на наборе серверов выполняют идентичные копии одного состояния и могут продолжать выполнение, даже если один из серверов недоступен. Реплицируемые конечные автоматы используются для того, чтобы решить различные проблемы допустимости отказов в распределённых системах. Для примера, система, в которой присутствует один кластер-лидер, такая как GFS [8], HDFS [38] или RAMCloud [33] обычно использует отедльный реплицируемый конечный автомат для управления избранием лидера и хранения конфигурационной информации чтобы не зависить от отказов лидера. Примерами реплицируемых конечных автоматов являются Chubby [2]
и ZooKeeper [11].

Реплицируемые конечные автоматы обычно описываются с помощью реплицируемого журнала, как показано на рис. 1. Каждый сервер хранит журнал, в котором содержится набор команд, для выполнения конечного автомата на этом сервере. Каждый журнал содержит одинаковые команды в одинаковом порядке. Поэтому каждый конечный автомат выполняет одинаковую последовательность команд. Поскольку конечные автоматы детерминированы, каждое выполнение приводит к одинаковому состоянию и одинаковой последовательности результатов.

Отслеживание целостности реплицируемого журнала это задача алгоритма конценсусов. Модуль конценсусов на сервере получает команды от клиентов и добавляет их в журнал. Затем он устанавливает связь с модулями конценсуса на других серверах, чтобы убедиться, что каждый журнал в конечном итоге содержит одинаковые запросы в одинаковом порядке, даже в случае отказа некоторых серверов. Как только команды полностью реплицированы, каждый конечный автомат на серверах обрабатывает их в указанном порядке и результат возвращается клиенту. В итоге сервера выглядят как единый безотказный конечный автомат.

Алгоритмы конценсуса для практического применения обычно имеют следующие свойства:

* Они дают гарантию надёжности (никогда не возвращается неверный результат) при любых non-Byzantine условиях, включая задержки сети, разделение и потерю пакетов, дублирование и изменение порядка пакетов.
* Они полностью работоспособны (доступны) до тех пор, пока большинство серверов находятся в рабочем состоянии и могут взаимодействовать друг с другом и с клиентами. Таким образом, кластер из пяти серверов способен работать при выключении двух серверов. Сервер считается неработоспособным при его выключении; позже он может быть переведён в рабочее состояние и включён в кластер.
* Целостность журнала не зависит от синхронизации времени. В худшем случае, при очень сильной рассинхронизации времени возможны проблемы с доступностью кластера.
* В общем случае команда может быть выполнена как только большинство в кластере доступно в течении одного раунда удалённых вызовов процедур; меньшинство более медленных серверов не должно влиять на общую производительность системы.

## 3. Что не так с Paxos?

В течении последних десяти лет, протокол Paxos Лесли Лампорта [15] стал синонимом конценсуса: об этом протоколе восновном говорят на курсах и большинство реализаций конценсуса используют его как точку отпарвления. Вначале Paxos определяет протокол для достижения согласия по _единственному решению_, например для одной записи в реплицируемом журнале. Мы называем это _(single-decree Paxos)_. Затем Paxos комбинирует несколько экземпляров данного протокола для принятия серии решений, таких как весь журнал _(multi-Paxos)_. Paxos гарантирует надёжность и живучесть и поддерживает изменения состава кластера. Его корректность была доказана и в обычных случаях это эффективный протокол.

К сожалению, Paxos имеет ряд серьёзных недостатков. Первым из них является очень высокая сложность для понимания. Полное описание [15] известно своей непонятностью; небольшое количество людей приложив массу усилий могут понять о чём там говориться. В результате было сделано несколько попыток описать Paxos в более простых терминах [16, 20, 21]. Эти описания восновном касаются single-decree Paxos и до сих пор в них вносятся изменения. В информационном обзоре участников NSDI 2012 в числе исследователей мы нашли лишь несколько человек, которые хорошо разбирались в Paxos. Мы сами испытывали большие проблемы с Paxos; мы не смогли полностью понять протокол даже после прочтения нескольких упрощённых описаний и разработки нашего альтернативного протокола &mdash; весь этот процесс занял у нас почти год.

Мы предположили, что такая сложность Paxos обусловлена тем, что за его основу взято подмножество single-decree. Paxos single-decree разбит на две стадии, для которых нет простого интуитивного описания и которые невозможно понять по отдельности. Композиционные правила для multi-Paxos добавляют сложности. Мы уверены, что общая проблема при достижении конценсуса для нескольких решений (т.е. всего журнала вместо записей в журнале) может быть разбита на составляющие другими, более непосредственными и понятными способами.

Вторая проблема Paxos в том, что он не предоставляет приемлемой базы для построения практических реализаций. Одна из причин в том, что не существует общепринятого алгоритма для multi-Paxos. Описание Лампорта восновном касается single-decree; он приводит в общих чертах подходы для multi-Paxos, но во много деталей опущено. Было предпринято несколько попыток конкретизировать и оптимизировать Paxos [26, 39, 13], но они отличаются друг от друга и от набросков Лампорта. Такие системы, как Chappy [9] описывают похожие на Paxos алгоритмы, но в основном их детализация не публикуется.

Более того, архитектура Paxos не подходит для построения практических систем; это ещё одно следствие разбиения single-decree. Например, можно получить небольшую выгоду, выбирая набор записей журнала независимо и затем объединяя их в последовательный журнал; но это добовляет сложность системы. Проще и эффективнее разработать для журнала систему, где новые записи добавляются последовательно в принудительном порядке. Другая проблема заключается в том, что Paxos в своём ядре использует симметричный peer-to-peer (хотя в конечном счёте это предполагает оптимизацию быстродействия засчёт слабой формы лидерства). Данный подход актуален в упрощённом мире, где необходимо принять единственное решение, однако его используют несколько практических применений. В случае, когда необходимо принять последовательность решений, проще и быстрее сначала выбрать лидера и затем позволить ему координировать решения.

В результате, практические применения имеют немного сходства с Paxos. Каждая реализация начинается с Paxos, обнаруживает трудности описания и затем разрабатывает полностью отличную архитектуру. Это отнимает много времени и привносит ошибки. Формулмровка Paxos может быть неплоха для доказательства теорем о его корректности, но в реализации настолько отличаются от Paxos, что данные доказательства не имеют особого значения. Приведём типичный комментарий:

> There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.... the final system will be based on an unproven protocol [4].

В свете описанных проблем мы решили, что Paxos не является подходящей базой для построения систем или даже для изучения. При том, что в больших системах алгоритм конценсуса является довольно важным, мы решили постараться разработать альтернативный алгоритм конценсуса, обладающий лучшими чем у Paxos свойствами. Результатом этого эксперимента и является Raft.

## 4. Разработан для ясности

У нас есть несколько целей при разработке Raft: он должен предоставлять полную и практичную базу для построения систем, поэтому значительно сокращено количество проектирования, требуемого от разработчиков; он должен быть надёжным при любых условиях и доступным при обычных условиях работы; также он должен быть эффективным для обычных операций. Но наша первостепенная и самая труднодостижимая цель &mdash; это _яность_ или понятность протокола. Самая широкая аудитория должна без особых проблем понимать алгоритм. Также алгоритм должен быть интуитивен для того, чтобы разработчики систем могли делать расширения, которые неизбежны в различных реализациях.

В разработке Raft было много моментов, когда было необходимо выбрать из нескольких альтернатив один подход. В таких ситуациях мы рассматривали альтернативы с точки зрения ясности: насколько сложно описать каждую альтернативу (например, насколько сложен её набор состояний и имеет ли она скрытый подтекст) и насколько просто читателю будет полность понять данный подход и его реализации?

Мы определили, что при таком анализе мы имеет высокую степень субъективности; тем не менее мы использовали два метода. Первый, это хорошо известный метод декомпозиции: где это возможно, мы разделяли проблемы на отедльные составляющие, которые могли быть решени, описаны и поняты полностью независимо друг от друга. К примеру, в Raft мы разделили выбор лидера, репликацию журналов, надёжность и изменение членства в кластере.

Нашим вторым методом было упрощение набора состояний чтобы сделать систему более понятной и устранить неопределённость везде, где это возможно. К примеру, журналы не могут иметь скважность и Raft ограничивает набор возможных путей, по которым журнал может утратить целостность. Мы старались убрать неопределённость, однако существует несколько ситуаций, когда неопредлённость повышает ясность. В частности, рандомизация привносит неопределённость, но она нужна для уменьшения количества состояний, поскольку обрабатывает любые возможные решения одинаковым способом ("выберай любой &mdash; нам не важно"). Мы используем рандомизацию для упрощения алгоритма выбора лидера.

## 5. Алгоритм конценсуса Raft

Raft это алгоритм для управления реплицируемым журналом в форме, описанной в главе 2. На рис. 2 показан алгоритм в сжатом виде, на рис. 3 приведён список ключевых свойств алгоритма; элементы данных рисунков будет обсуждаться отедльно в этой главе.

Для достижения конценсусв в Raft вначале выбирается _лидер_. Затем лидеру делегируется ответсвенность за реплицирование журнала. Лидер принимает записи журнала от клиентов, реплицирует их на другие сервера, и оповещает сервера о том, когда можно безопасно применить записи журнала к конечным автоматам. Наличие лидера упрощает управление реплицируемым журналом. К примеру, лидер может решить в какое место журнала поместить новые записи без необходимости согласовывать это с другими серверами. При этом потоки данных сводяться к передаче с лидера на другие сервера. Лидер может выйти из строя или отключиться. В этом случае выбирается новый лидер.

При таком подходе Raft разделяет проблему конценсуса на три независимых проблемы, которые будут описаны ниже:

* **выбор лидера:** при отказе существующего лидера должен быть назначен новый лидер (раздел 5.2).
* **репликация журнала:** лидер должен принимать записи журнала от клиентов и реплицировать их в кластер, приводя журналы на всех серверах к данному состоянию (раздел 5.3).
* **надёжность:** ключевое свойство надёжности для Raft это свойство надёжности кончного автомата, изображённое на рис. 3: если какой-либо сервер применил определённую запись журнала к своему конечному автомату, то ни один другой сервер не может применить никакую другую команду для этого же индекса журнала (номера записи). В разделе 5.4 описано, как это реализовано в Raft; решение включает дополнительное ограничение алгоритма выбора лидера, описанное в разделе 5.2

После описания алгоритма конценсуса в данной главе рассматривается проблема доступности и роль синхронизации времени в системе.

### 5.1. Основы Raft

Кластер Raft содержит некоторое количество серверов; типичное значение &mdash; 5. Данное количество позволяет системе продолжать работу при отказе двух серверов. В любой момент времени каждый сервер находится в одном из трёх состояний: _лидер_, _ведомый_ и _кандидат_. В обычных условиях в системе присутствует только один лидер, все остальные сервера являются ведомыми. Ведомые сервера пассивны: они не генерируют запросов и отвечают на запросы от лидера или кандидата. Лидер обрабатывает все запросы от клиентов (если клиент связывается с ведомым, ведомый перенаправляет его на лидера). Третье состояние, _кандидат_, используется для выбора лидера и описано в разделе 5.2. На рис. 4 показаны все состояния и переходы между ними. Все переходы обсуждаются в данном разделе ниже.

Raft разделяет время на _интервалы_произвольной длины, как это показано на рис. 5. Интервалы аоследовательно пронумерованы с помощью целых чисел. Каждый интервал начинается с _выборов_, в которых один или несколько кандидатов пытаются стать лидерами, как это описано в разделе 5.2. Если кандидат выигрывает виборы, он становится лидером на весь данный верменной интервал. В некоторых случаях выборы дают разделённое голосование. В этом случае интервал заканчивается без лидера и сразу создаётся новый интервал (с новыми выборами). Raft гарантирует, что в одно и тоже время существует только один лидер.

Сервера в кластере отслеживают момент смены интервала и делают это в различные моменты времени, и в некоторых случаях, сервер может пропустить момент выборов или, даже, весь интервал полностью. Интервалы в Raft являются логическими часами [14] и позволяют серверам получать устаревшую информацию, такую как прошлый лидер. Каждый сервер хранит номер _текущего интервала_, который увеличивается со временем. При взаимодействии серверов, происходит обмен текущими интервалами; если у одного из сервером текущий интервал меньше, чем у другого, то сервер с меньшим текущим интервалом увеличивает его до большего зачения. Если кандидат или лидер выисняет, что его текущий интервал просрочен, он тутже становится ведомым. Если сервер получает запрос с просроченным интервалом, данный запрос игнорируется.

Взаимодействие между серверами осуществляется с помощью удалённого вызова процедур (RPC) и для реализации основного алгоритма конценсуса необходимо только два типа RPC. RPC RequestVote, который вызывается кандидатами в момент выборов (раздел 5.2) и RPC AppendEntries, который вызывается лидером для репликации записей журнала (раздел 5.3). В главе 7 добавляется ещё один RPC для передачи снимков между серверами. Если сервер, вызывав RPC, не получил своевременный ответ, запрос повторяется. Для улучшения производительность RPC могут выполнляться на серверах параллельно.

### 5.2. Выборы лидера

Для инициации выбора лидера в Raft используется механизм heeartbeat. При запуски сервера начинают свою работу, как ведомые. Сервер остаётся в режиме ведомого до тех пор, пока он получает верные RPC запросы от кандидата. Лидер периодически уведомляет ведомых о том, что он в рабочем состоянии (с помощью пустого RPC AppendEntries). Если в течение определённого времени, называемого _таймаут выборов_ ведомый не получает данного подтверждения, то считается, что лидер отсутствует и начинаются выборы нового лидера.

Для начала выборов, ведомый увеличивает текущий интервал и переходит в режим кандидата. Затем он голосует сам за себя и параллельно выполняет RPC запрос RequestVote к каждому серверу в кластере. Кандидат находится в данном состоянии пока непроизойдёт одно из следующих событий, описанных ниже в данной главе:

* он выиграет выборы
* другой сервер назначит себя как лидера
* наступит таймаут и при этом не будет выигравшего

Кандидат выигрывает выборы, если он получает большинство голосов от сервов в данном кластере в конкретный временной интервал. Кажды сервер может проголосовать только за одного кандидата в конкретный интервал времени. Голосование выполняется по принципу "первый пришёл, первый обслужен" (в разделе 5.4 описываются дополнительные ограничения на голосование). Правило большинства гарантирует, что только один кандидат может выиграть выборы в определённый период времени (свойство надёжности выборов на рис. 3). Как только кандидат выигрывает выборы он становится лидером. Далее он посылает hertbeat сообщения всем серверам чтобы подтвердить свои полномочия и предотвратить новые выборы.

Ожидая голоса от серверов кандидат может получить RPC AppendEntries от другого сервера, желающего стать лидером. Если интервал лидера (указанный в RPC) не меньше текущего периода кандидата, то кандидат признаёт лидера и переходит в режим ведомого. Если интервал в RPC меньше текущего интервала кандидата, то он отбрасывает данный запрос и продолжает работать в режиме кандидата.

Третий возможный случай когда кандидат не выигрывает и не проигрывает выборы: если много ведомых становятся кандидатами в одно и тоже время, голоса могут распределиться таким образом, что ни у одного из кандидатов не будет большинства голосов. В этом случае каждый кандидат инициирует новые выборы, увеличивая номер своего интервала и посылая новый набор RPC запросов RequedstVote. Однако без дополнительных действий, разделение голосов может повторяться бесконечно.

Raft использует случайные таймауты выборов для гарантии, что разделение голосов будет происходит редко и данная ситуация будет разрешаться быстро. Для предотвращения разделения голосов сначала используется случайно выбранный из фиксированного интервала (150-300 мс) таймаут выборов. Таким образом сервера разносятся во времени и в большинстве случаев только на одном сервере произойдёт таймаут выборов и он инициирует и выиграет выборы и пошлёт heartbeat пакеты до того, как таймаут выборов наступит на других серверах. Подобный механизм испольлзуется и для разрешения разеделения голосов. Каждый кандидат перезапускает свой случайный счётчик в начале выборов и ждёт в течении этого случайного периода прежде чем начать новые выборы; это снижает верочтность разделения голосов в следующих выборах. В главе 9.3 показано, что с помощью данного подхода лидер выбирается очень быстро.

Выборы это один из примеров того, как ясность или понятность различных подходов влияет на выбор этих подходов при разработке Raft. Изначально мы планировали использовать систему рангов: каждый кандидат получал свой уникальный ранг, который затем использовался для выбора между кандидатами. Если кандидат обнаруживал другого кандидата с большим рагом, он возвращался в режим ведомого, а кандидат с максимальным рангом выигрывал выборы. Однако, мы обнаружили, что данный подход имеет скрытые проблемы с доступностью (сервер с меньшим рангом должен снова стать кандидатом в случае, если сервер с более высоким рангом откажет, однако, если это произойдёт слишком поздно, то весь процесс может сбросться до выбора лидера). Вы делали несколько попыток настроить данный алгоритм, но при каждой настройке возникали новые частные случаи данной ситуации. В конце концов мы решили, что болше подхоит вариант со случайными попытками и что он более понятен.

### 5.3. Репликация журнала

Как только выбран лидер он начинает обслуживать запросы от клиентов. Кажды запрос от клиента содержит команду, которая будет выполнена с помощью реплицируемых конечных автомтов. Лидер добавляет команду как новую запись в свой журнал и затем параллельно делает RPC запросы AppendEntries ко всем серверам в кластере для того, чтобы реплицировать запись. Когда запись гарантированно реплецирована (как описано ниже), лидер применяет запись к своему конечному автомату и возвращает результат выполнения клиенту. Если видомые вышли из строя или работают медленно или в сети имеются потери пакетов, лидер повторяет AppendEntries RPC вызовы бесконечно (даже после того, как он уже ответил клиенту) до тех пор, пока все ведомые в конечном счёте не сохранят эту запись.

Организация журнала показана на рис. 6. Каждая запись журнала хранит команду конечного автомата и номер интервала, когда запись получена лидером. Номер интервала в записях журнала используется для выявления несоответсвий между журналами и для гарантии некоторых свойств, показаных на рис. 3. Каждая запись журнала также имеет целочисленный индекс, поределяющий её место в журнале.

Лидер решает, когда можно безопасно применить записи журналов к конечным автоматам; такие записи называются _применённые_. Raft гарантирует, что применённые записи надёжны и в конечном счёте будут выполненны на всех доступных конечных автоматах. Запись журнала становится применённой как только лидер, создавший данную запись реплицирует её на большинство серверов (запись №7 на рис. 6). В этом случе также применёнными считаются все записи, предшествующие данной в журнале лидера, включая записи, созданные предыдущими лидерами. В разделе 5.4 осбсуждаются некоторые трудности, связанные с применением этого правила после смены лидера, также там показано, что данное предположение о применённости надёжно. Лидер отслеживает наибольший индекс команды, которая должна быть применена и включает этот индекс во все последующие вызовы AppendEntries (включая вызовы hertbeat). Как только ведомый понимает, что запись применённая, он выполняет её на локальном конечном автомате (в порядке, указанном в журнале).

Журналы Raft разработаны таким образом, чтобы можно было сохранять высокую степень связанности между журналами на разных серверах. Это не только упрощает поведение системы и делает её более предсказуемой, но также является важной частью гарантантирования надёжности. Raft обеспечивает следующие свойства, которые в совокупности дают свойство соответствия журналов на рис. 3:

* если две записи в различных журналах имеют одинаковый индекс и интервал, то в них хранится одна команда.
* если две записи в различных журналах имеют одинаковый индекс и интервал, то все предыдущие записи в журналах идентичны.

Первое утверждение следует из того, что лидер создаёт только одну запись с данным индексом в конкретном интервале и из того, что записи никогда не меняют свою позицию в журнале. Второе свойство обеспечивается простой проверкой целостности, выполняемой в AppendEntries. При вызове AppendEntries, лидер включает индекс и интервал в своём журнале, которые непосредственно предшествуют новым записям. Проверка целостности действует как вводный шаг (induction step): изначально пустой журнал удовлетворят свойству соответствия журналов и проверка целостности сохраняет это свойство при расширении журнала. В результате, если AppendEntires выполнена полностью, лидер знает, что журнал ведомого идентичен его собственному вплоть до последней записи.

При нормальном функционировании журналы лидера и ведомых остаются целостными и проверка целостности в AppendEntries проходит успешно. Однако, сбой лидера может привести к потере целостности журналов (предыдущий лидер может иметь журнал с не полностью реплицированными записями). Данная ситуация может посторяться в результате последовательных отказов лидера и ведомых. На рис. 7 показаны варианты, при которых журналы ведомых могут отличаться от журнала нового лидера. Ведомый может пропустить записи, которые присутствуют у лидера и может иметь дополнительные записи, которых нет на лидере или возможны оба этих случая. Отсутствующие или лишние записи могут появляться в нескольких временных интервалах.

В Raft обработкой таких несоответствий занимается лидер, заставляя журналы ведомых дублировать журнал лидера. Это означает, что конфликтующие записи в журнале ведомого будут перезаписаны записями из журнала лидера. В разделе 5.4 будет показано, что с некоторыми ограничениями такой подход является надёжным.

Для приведения журнала ведомого в соответсвие с журналом лидера, лидер должен найти последнюю запись, до которой журналы совпадают, затем удалить все последующие записи из журнала ведомого и послеать ведомому все записи, начиная с найденной. Все эти действия происходят при запросе проверки целостности, выполняемой в AppendEntries. Лидер хранит индекс следующей записи _nextIndex_ для каждого ведомого. Когда лидер запускается, он инициализирует все значения nextIndex индексом, следующей записи в собственном журнале (11 на рис. 7). Если журнал ведомого не соответствует журналу лидера, проверка целостности AppendEntries завершится неудачно при следующем вызове этой функции. После этого лидер уменьшит nextIndex и повторит запрос AppendEntries. В конце концов nextIndex достигнет заприси одинаковой на лидере и на ведомом. После этого AppendEntries будет выполнена успешно, в следствии чего на ведомом удалятся все конфликтующие записи и добавяться все записи с лидера (если таковые имеются). Как только AppendEntries выполнена успешно, журналы лидера и ведомого совпадают и данное утверждение останется верным до конца периода.

Если это необходимо, протокол может быть оптимизирован путём уменьшения неудачных вызовов AppendEntries. Для примера, при неудачном запросе AppendEntires ведомый может включить в ответ номер интервала для конфликтующей записи и первый индекс для данного интервала. Обладая этой информацией лидер может уменьшать nextIndex не на единицу, а до первого индекса в интервале; таким образом будет необходим один вызов AppendEntries на интервал с конфликтующими записями вместо одного вызова на запись. Мы смоневаемся, что данная оптимизация необходима на практике, поскольку такие несоответствия записей происходят не часто и при этом появляется небольшое число конфликтующих записей.

Используя данный механизм лидер избавляется от необходимости иметь специальную логику для восстановления целостности журналов при старте. Он просто начинает своё обычное функционирование и журналы автоматически приводятся в одинаковое состояние с помощью проверки целостности в AppendEntries. Лидер никогда не перезаписывает и не удаляет записи в своём журнале (свойство только добавления в журнал лидера на рис. 3).

Данный способ репликации журналов предоставляет необходимые свойства конценсуса, описанные в главе 2: Raft может принимать, реплицировать и применять новые записи журнала до тех пор, пока большинство серверов находится в рабочем состоянии; в обычном случае одна запись может быть реплицирована за один раунд RPC запросов к большинству серверов в кластере; один медленный ведомые не влияет на общую производительность системы.

### 5.4. Надёжность

В предыдущих разделах показано, как Raft выбирает лидера и реплицирует записи журналов. Однако, описанных механизмов недостаточно для гарантии того, что каждый конечный автомат выполнит абсолютно одинаковую последовательность команд в одинаковом порядке. Для примера, ведомый может быть не доступен в тот момент, когда лидер отправляет некоторые записи журнала, а затем он (ведомый) может быть выбран как лидер и перезапишет данные записи другими; в результате различные конечные автоматы могут выполнить различные последовательности команд.

Данная глава завершает описание алгоритма Raft добавляя ограничивающие условия, при которых сервер может стать лидером. Эти ограничения гарантируют, что лидер для любого временного интервала содержит все применённые записи из предыдущих интервалов (свойство полноты лидера на рис. 3). Имея ограничение на избрание мы затем уточним правила применения записей. В конце мы покажем пример свойства полноты лидера и покажем, как оно корректирует поведение реплицируемых конечных автоматов.

#### 5.4.1. Ограничение избрания

В любом алгоритме конценсуса, основанном на лидере, лидер должен в конечном счёте хранить все применённые записи журнала. В некоторых алгоритмах конценсуса, таких как Viewstamped Replication [22] лидер может быть избран даже если он изначально не содержит всех применённых записей. Такие алгоритмы содержат дополнительную логику для поиска отутствующих записей и передачи их новому лидеру непосредственно в момент избрания или сразу после этого. К сожалению это увеличивает сложность системы. Raft использует более простой подход, в котором гарантируется, что все применённые записи в предыдущих интервалах присутствуют на любом новом лидере с момента его избрания без необходимости передавать такие записи лидеру. Это означает, что записи журнала передаются только в одном направлении &mdash; от лидера к ведомым и лидер никогда не перезаписывает записи в журнале.

Raft использует процесс голосования чтобы недопустить выбор кандидата лидером до тех пор, пока его журнал не будет содержать все применённые записи. Чтобы быть избраным, кандидат должен получить большинство голосов в кластере, что означает, что каждая применённая запись должна существовать по крайней мере на одном из серверов, отдавших свой голос за кандидата. Если журнал кандидата такой же или более свежий в сравнении с любым жруналом из большинства (понятие "более свежий" будет рассмотрено ниже), то это означает, что он хранит все применённые записи. Данное ограничение описывается в RPC запросе RequestVote: при вызове передаётся информация о журнале кандидата и голосующий не отдаёт свой голос за кандидата, если его журнал свежее журнала кандидата.

Raft определяет, какой журнал более свежий сравнивая индекс и номер интервала последней записи в журнале. Если два журнала имеют различные интервалы, то журнал с последним интервалом считается более свежим. Если интервалы совпадают, журнал с большим количеством записей считается более свежим.

#### 5.4.2. Применение записей из предыдущих интервалов

Как описано в разделе 5.3 лидер знает, что однажды применённая запись в текущем интервале сохранена на большинстве серверов. Если на лидере произойдёт сбой до того, как записи будут применены, будущие лидеру будут пытаться закончить репликацию данной записи. Однако, лидер не может сразу решить, является ли запись из предыдущего интервала применённой если она сохранена на большинстве серверов. На рис. 8 приведена ситуация, когда старая запись журнала сохранённая на большинстве серверов может быть перезаписана следующим лидером.

Для исключения подобных проблем в Raft никогда не применяются записи журналов из предыдущих интервалов при помощь подсчёта реплик. Только записи журнала к текущем интервале лидера могут быть применены методом подсчёта реплик; как только запись из текущего интервала применена таким образом, все предыдущие записи применяюстя косвенно исходя из свойства соответствия журналов. Существуют ситуации, когда лидер может безопасно предположить, что старая запись журнала применена (для примера, если запись сохранена на каждом сервере), но Raft использует более консервативный подход для простоты.

Raft исключает данное усложнение в правилах применения, поскольку записи журнала хранят изначальные номера интервалов на момент, когда лидер реплицирует записи из предыдущих интервалов. В других алгоритмах конценсуса если новый лидер реплицирует записи из предыдущих интервалов, он должен делать это с новым номером интервала. Raft облегчает этот механизм, поскольку записи журнала хранят тотже одинаковый номер интервала независимо от времени и конкретного журнала. В дополнении в Raft новые лидеры шлют меньшее количество записей из предыдущего интервала, чем в других алгоритмах (другие алгоритмы вынуждены слать лишние записи чтобы перенумеровать их перед применением).

#### 5.4.3. Аргументация надёжности

Имея полный алгоритм Raft, мы теперь можем точнее говорить о том, присутствует ли свойство полноты лидера (данный аргумент основывается на докозательстве надёжности; см. главу 9.2). Мы предполагаем, что свойство полноты лидера не присутствует если мы докажем обратное. Допустим, лидер для интервала T (leader-T) применяет запись журнала в данном интервале, но эта запись не сохраняется лидером некоторого будущего интервала. Предположим мы имеем, малый интервал U, при этом U > T (интервал U следует за интервалом T), лидер которого (leader-U) не хранит эту запись.

1. Применённая запись должна отсутствовать в журнале лидера U на момент его избрания (лидеры никогда не перезаписывают и не удаляют записи).
2. Лидер T реплицировал запись на большинство серверов кластера. Таким образом, по карйней мере один сервер (голосующий) и принял запись от лидера T и голосует за лидера U, как показано на рис. 9. Голосующий в данном случае является ключевым для достижения противоречия.
3. Голосующий должен иметь применённую запись от лидера T _перед_ тем, как он будет голосовать за лидера U; иначе он отклонит запрос AppendEntries от лидера T (его текущий интервал будет выше, чем интервал T).
4. Голосующий сохраняет запись на момент голосования за лидера U, поскольку любой предполагаемый лидер содержит запись (по определению), лидеры никогда не удаляют записи и ведомые удаляют записи только в случае конфикта с лидерами.
5. Голосующий предоставляет свой голос лидеру U в случае, если журнал лидера U такой же или более свежий, чем журнал голосующего. Из этого следует одно или два противоречия.
6. Во-первых, если у голосующего и лидера U одинаковый номер интервала, то журнал лидера U должен быть той же длины, что и журнал голосующего. Однако это невозможно, поскольку журнал голосующего содержит применённую запись, а журнал лидера U нет.
7. В другом случае, интервал последней записи в журнал у лидера U должен быть больше, чем интервал голосующего. Более того, он должен быть больше чем T, поскольку интервал последней записи в журнал у голосующего был как минимум T (он содержит применённую запись из интервала T). Предыдущий лидер, который создал последнюю запись в журнале лидера U должен был иметь эту применённую запись в своём журнале (по определению). Теперь, следуя свойству соответсвия журналов, журнал лидера U должен также содержать применённую запись, что и является противоречием.
8. Таким образом мы описали полное противоречие. Т.е. все лидеры для интервалов после T должны содержать все записи из интервала T, которые были применены в интервале T.
9. Свойство соответствия журналов гарантирует что будущие лидеры также будут содержать записи, которые были применены косвено, такие как с индексом 2 на рис. 8 (d).

Имея свойство применённого лидера мы можем доказать свойство надёжности конечных автоматов (рис. 3), которое указывает, что если сервер применил запись журнала с определённым индексом, то ни один другой сервер никогда не применит другую запись журнала с тем же индексом. На момент, когда сервер применяет запись журнала к своему конечному автомату, его журнал должен быть идентичен журналу лидера вплоть до данной записи и данная запись должна быть применённой. Теперь предоложим, что имеется некий предыдущий интервал, в котором некий сервер применил данный индекс; свойство соответствия журналов гарантирует, что лидеры для всех последующих интервалов будут хранить эту одинаковую запись журнала и сервера, которые будут применять этот индекс в будущих интервалах применят одинаковое значение. Что означает, что свойство надёжности конечных автоматов присутствует.

В заключении, Raft требует от серверов применять записи в порядке индекса. Принимая во внимание свойство надёжности конечных автомтов, это означает, что все сервера применят абсолютно одинаковую последовательность записей журнала к своим конечным автоматам в одинаковом порядке.

### 5.5. Отказы ведомых и кандидатов

До данного момента мы рассматривали отказы лидеров. Отказы ведомых и кандидатов обрабатывать проще, чем отказы лидеров и оба этих отакза обрабатываются одинаковым образом. Если произошёл отказ ведомого или кандидата, то следующие запросы RequestVote и AppendEntries закончатся неудачей. В Raft такие неудачные запросы повторяются бесконечно; если отказавший сервер перезапущен, то все запросы RPC будут успешно завершены. Если сервер отказал до завершения вызова но после того, как он уже ответил на запрос, то он получит этот же запрос после перезапуска. RPC в Raft иденпотентны (idempotent), поэтому такие повторения не являются проблемой. Для примера, если ведомый получит запрос AppendEntries, который содержит записи журнала, которые уже существуют в журнале, то он просто проигнорирует такие записи в следующих запросах.

### 5.6. Синхронизация и доступность

Одно из условий Raft заключается в том, что надёжность не должна зависеть от инхронизации: система не должна возвращать неправильный результат просто из-за того, что события происходят быстрее или медленнее, чем это может ожидаться. Однако, доступность (возможность системы своевеременно отвечать клиентам) неизбежно будет зависеть от синхронизации. Для примера, если обмен сообщениями запнимает больше времени чем обычно требуется для перезапуска после отказа сервера, кандидаты не смогут ожидать достаточно времени для победы в выборах; без существующего лидера Raft не сможет функционировать.

Выбор лидера, это та часть Raft, где синхронизация наиболее сильно влияет на процесс. Raft способен выбрать и сохранять лидера настолько долго, пока система удовлетворяет следующему _условию синхронизации_:

broadcastTime << electionTimeout << MTBF

в данном уравнении _broadcastTime_ это среднее время, затрачиваемое сервером на параллельную посылку RPC запроса каждому серверу в кластере и получение ответов от них; _electionTimeout_ это таймаут выборов, описанный в разделе 5.2; и _MBTF_ это среднее время между отказами для одного сервера. _broadcastTime_ должно быть на порядок меньше таймаута выборов чтобы лидеры могли надёжно рассылать heartbeat сообщения, необходимые для предотвращения начала выборов ведомыми; с учётом принципа случайных таймаутов, данное неравенство также предотвращает разделение голосов. Таймаут выборов должен быть на несколько порядков меньше величины _MBTF_ для того, чтобы система стабильно работала. При сбое лидера система будет недоступна примерно в течении таймаута выборов.

_broadcastTime_ и _MBTF_ это свойства системы, но таймаут выборов мы можем и должны назначить. Raft RPC обычно требует от получателся сохранять информацию в надёжном хранилище, поэтому время броадкаста может варьироваться от 0,5 до 20 мс в зависимости от технологии хранения данных. В результате таймаут выборов должен быть в пределах между 10 и 500 мс. _MBTF_ обычно равен нескольким месяцам или больше, что также удовлетворяет приведённым условиям.

## 6. Изменение состава учатсников кластера

До сих пор мы предполагали, что _конфигурация_ кластера (набор серверов, участвующих в алгоритме конценсуса) фиксирована. На практике иногда необходимо менять эту конфигурацию, например, для замены серверов или изменения степени репликации. Данную процедуру можно выполнить полностью отклчив весь кластер, затем обновив конфигурационные файлы и вновь его запустить, при этом на всё время внесения изменений кластер будет недоступен. В добавок, на некоторых этапах возможны ошибки оператора. Чтобы исключить эти проблемы мы решили автоматизировать изменения конфигурации и включить их в алгоритм конценсуса Raft.

Чтобы механизм изменения конфигурации был надёжным, в процессе перехода не должно быть момента, когда будут выбраны два лидера для одного интервала. К сожалению, любой подход, при котором червера непосредственно меняют конфигурацию не надёжен. Невозможно автоматически переключить все сервера одновременно и в процессе перехода кластер потенциально может разделиться на два независимых большинства (см. рис. 10)

Поэтому для гарантии надёжности изменения конфигурации должны проходить в два этапа. Есть несколько способов описать эти два этапа. Например, некоторые системы [22] в первом этапе отключают старую конфигурацию, чтобы в неё не обслуживались запросы клиентов; затем, во втором этапе включается новая конфигурация. В Raft кластер сначала переключается на промежуточную конфигурацию, которую мы называем _соединительный конценсус_; как только соединительный конценсус применён система переходит к новой конфигурации. Соединительный конценсус совмещает старую и новую конфигурации:

* Записи журналов реплицируются на все сервера в обеих конфигурациях
* Любой сервер из любой конфигурации может стать лидером
* Согласие (для выборов и для применения записей) требует отдельно большинства из обеих конфигураций.

Соединительный конценсус позволяет определённым серверам переходить от одной конфигурации к другой в различное время без влияния на надёжность. Более того, такой подход позволяет кластеру обслуживать запросы от клиентов непосредственно в процессе смены конфигурации.

Конфигурации кластера хранятся и передаются с помощью специальных записей в реплицируемом журнале; на рис. 11 показан процесс смены конфигурации. Когда лидер получает запрос о смене конфигурации с C-old на C-new, он сохраняет конфигурацию для соединительного конценсуса (на рис. C-old,new) как запись в журнале и реплицирует эту запись использую описанный выше способ. Как только какой-либо сервер добавляет запись с новой конфигурацией в свой журнал, он использует эту конфигурацию для всех последующих решений (сервер всегда использует последнюю конфигурацию из своего журнала независимо от того, применена эта запись или нет). Это означает, что лидер для решения о том, применена ли запись конфигурации C-old,new также использует конфигурацию C-old,new. Если в этот момент на лидере происходит сбой, то новый лидер может быть выбран либо в конфигурации C-old либо в C-old,new, в зависимости от того, получил ли уже кандидат запись C-old,new. В любом случае, на данном этапе в конфигурации C-new не может быть принято односторонних (безвозвратных) решений.

Как только C-old,new применена, ни C-old, ни C-new не могут быть использованы для принятия решения без подтверждения друг друга и свойство полноты лидера гарантирует, что теперь лидером может быть выбран только сервер, имеющий запись C-old,new. Теперь лидер может безопасно создать запись C-new и реплицировать её в кластер. И снова эта конфигурация будет применена на каждом сервере, как только он получит данную запись. Когда новая конфигурация будет применена по правилам C-new, старая конфигурация уже не нужна и сервера, не участвующие в новой конфигурации могут быть отключены. Как показано на рис. 11, что нет такого периода времени, когда в обеих конфигурациях C-old и C-new могут быть приняты односторонние решения; это гарантирует надёжность.

Есть ещё три момента, связанных с изменением конфигурации. Во-первых, возможен вариант, когда новый сервер не имеет записей журнала. Когда такой сервер добавляется в кластер, ему необходимо некоторое время, чтобы подхватить записи, в течении этого времени сервер может не применять новые записи. Чтобы предотвратить такие интервалы недоступности в Raft предуссмотрен дополнительный этап перед сменой конфигурации, в котором новые сервера присоединяются к кластеру как неголосующие члены (лидер реплицирует на них записи, но они не учитываются в большинстве). Как только сервер синхронизировался с кластером, конфигурация может быть применена, как описано выше.

Во-вторых, может оказаться так, что текущий лидер кластера не является частью новой конфигурации. В этом случае лидер возворащается в состояние ведомого после того, как он применит запись C-new. Это означает, что будет некоторый интервал вермени (в процессе применения C-new), когда лидер управляет кластером, который не включет в себя этого лидера; он реплицирует записи, но не сам не учитывается в большинстве. Смена лидера происходит после применения C-new, поскольку это первый момент, когда новая конфигурация может работать независимо (в любой момент возможен выбор лидера из конфигурации C-new). До этого момента возможны варианты, когда может быть выбран только сервер из конфигурации C-old.

И в-третьих, отсоединённый сервер (которого не в C-new) может разрушить кластер. Такие сервера не получают сообщения heartbeat, поэтому по истечению таймаута они начинают новые выборы. Затем они шлют запросы RequestVote с новыми номерами интервалов и это заставит текущего лидера вернуться в режим ведомого. В любом случае будет выбран новый сервер, но отсоединённые сервера снова достигнут таймаута и процесс повторится, в результате мы получим плохую доступность.

Для предотвращения данной проблемы сервера игнорируют запросы RequestVote если они уверены, что текущий лидер существует. Говоря точнее, если сервер получает запрос RequestVote в течении минимального таймаута выборов, при этом получая heartbeat от текущего лидера, то он (сервер) не обновляет свой номер интервала и не голосует по такому запросу. Это не влияет на процесс обычных выборов, при которых каждый сервер ждёт в течении минимального таймаута выборов перед тем, как начинать выборы. Однако, это помогает предотвратить разрушения от отсоединённого сервера: если лидер может отправлять в кластер сообщения heartbeats, то он не будет свергнут большими номерами интервалов.

## 7. Уплотнение журналов

В процессе работы журналы Raft увеличиваются чтобы вместить новые запросы от клиентов, но в практических системах они не могут увеличиваться бесконечно. Чем дольше увеличивается журнал, тем больше места необходимо для его хранения и тем больше времени требуется для его просмотра. Это может вызвать проблемы доступности без специального механизма для удаления устаревшей информации, накапливающейся в журналах.

Создание снимков это наиболее простой метод уплотнения. При создании снимков всё текущее состояние системы сохраняется в надёжном хранилище, а затем весь журнал вплоть до этого момента удаляется. Снимки используются в Chubby и ZooKeeper и в этой главе будет расказано о снимках в Raft.

Существуют другие методы последовательного уплотнения, такие как очистка журанлов [36] и деревья слияния структуры журналов (log-structured merge trees) [30, 5]. Они работают с фрагментарными данными и поэтому процесс уплотнения распределён равномерно на всё время функционирования. Сначала выбирается участок данных, в котором накоплено большое количество удалённых или перезаписанных данных, затем неудалённые объекты переносятся из этого участка и затем он удаляется. В сравнении со снимками это требует значительно большего количества механизмов и большей сложности всей системы. Снимки работают с полным набором данных. Поскольку очистка логов требует модификации Raft, конечные автоматы могут реализовывать LSM-trees с помощью того же интерфейса, что и создание снимков.

На рис. 12 показана основная идея снимков в Raft. Каждый сервер делает снимки независимо, включая в журнал только применённые записи. Вся работа состоит в том, что конечный автомат записывает своё текущее состояние в снимок. Raft также включает в снимки некоторое количество метаданных: _последний включённый индекс_ &mdash; индекс последней записи журнала, которую заменяет снимок (последняя запись применённая к конечному автомату) и _последний включённый интервал_ &mdash; интервал этой записи. Это необходимо для механизма проверки целостности в AppendEntries для записи пришедшеё непосредственно после выполнения снимка. Для изменения состава участников кластера (глава 6) в нимке также сохраняется последняя конфигурация. Как только сервер выполнил снимок, он может удалить все записи вплоть до последнего включённого индекса, а также все предыдущие снимки.

Несмотря на то, что сервера выполняют снимки независимо, изредка лидер должен посылать снимки отстающим ведомым. Это происходит, если лидер уже удалил запись, которую нужно послать ведомому. Данная ситуация происходит довольно редко и не является обычным случаем: нормально работющие ведомые должны уже иметь эту запись. Однако, очень медленный ведомый или новый сервер, присоединяющийся к кластеру (глава 6) может её не иметь. В этом случае, чтобы привести ведомого к текущему состоянию лидер высылает ему снимок по сети.

Для этого лидер использует новый RPC запрос InstallSnapshot (см. рис. 13). Когда ведомый получает такой запрос, он должен решить, что делать с существующими в его журнале записями. Как правило, снимок содержит новую информацию, которой ещё нет в его журнале. В этом случае ведомый полностью удаляет свой журнал; журнал полностью заменяется снимком и может содержать неприменённые записи, которые будут конфликтовать со снимком. Однако, если ведомый получает снимок, который описывает лишь часть (начальную) его журнала (из-за повторной передачи или по ошибке), то записи, покрываемые снимком удаляются, а записи следующие за снимком считаются верными и должны быть обработаны.

Такой механизм отходит от принципа строгого лидера, поскольку ведомые могут получить снимок не имея информации о лидере. Однако, мы считаем, что данное отклонение оправдано. Если в процессе достижении конценсуса лидер отвергает спорные решения, то в момент снимка конценсус уже достигнут и спорных решений нет. Данные всё также идут в направлении от лидера к ведомым, просто ведомые теперь могут реорганизовывать свои данные.

У нас был ещё один вариант, при котором только лидер мог создавать снимок и затем рассылал этот снимок всем ведомым. Однако здесь есть два недостатка. Во-первых пересылка снимков всем ведомым может загрузить сеть и замедлить процесс создания снимков. Каждый ведомый уже имеет всю информацию, необходимую для выполнения снимка и, как правило, для сервера проще создать снимок локального конечного автомата, чем отсылать и принимать его по сети. Во-вторых, при таком подходе, реализация лидера будет сильно усложнена. Например, лидер должен будет слать снимки параллельно, при этом отсылая ещё и новые записи и при этом не блокируя запросы от клиентов.

Существует ещё два момента, касающихся эффективности выполнения снимков. Во-первых, сервер должен решить, когда выполнять снимок. Если делать снимки слишком часто, это загрузит пропускную способность диска и увеличит потребление энергии; если делать снимки слишком редко, есть риск достигнуть пределов дискового пространства, а также это увеличит время необходимое на промотку (replay) журнала при перезапуске севрера. Простейшая стратегия &mdash; выполнять снимки, когда физический размер журнала (в байтах) достигает некоторого фиксированного значения. Если это значение установить намного большим, чем ожидаемый размер снимка, то пропускная способность диска не будет сильно заполняться при выполнении снимка.

Вторая проблема в том, что запись снимка может занять значительное время и нам не нужны задержки для нормального функционирования. Решением может быть техника копирования при записи (copy-on-write) при которой обновления могут быть приняты без влияния на записываемый снимок. Например, конечные автоматы построены с функциональными структурами данных (functional data structures), изначально поддерживающими данный механизм. В качестве альтернативы может быть использована поддержка copy-on-write из операционной системы (например fork в Linux) для создания в оперативной памяти снимков всего конечного автомата (наша реализация использует именно такой подход).

## 8. Взаимодействие с клиентами

В данной главе описывается, как клиенты взаимодействуют с Raft, включая то, как клиенты находят лидера в кластере и как в Raft реализована линеаризуемая семантика (linearizable semantics) [10].

Клиенты Raft посылают все свои запросы лидеру. Когда клиент запускается первый раз, он подключается к случайно выбранному серверу. Если клиент выбрал не лидера, то выбранный сервер отклонит запрос и в ответе приложит информацию о последнем лидере, о котором ему известно (запросы AppendRequest содержат сетевой адрес лидера). Если на лидере произошёл сбой, клиент получит таймаут соединения; после этого клиент опять попытается подключиться к случайно выбранному серверу.

Наша задача описать линеаризуемую семантику (любая поступившая операция выполняется мгновенно, один и только один раз, в какой-то момент времени между её вызовом и ответом на нёё). Однако, как до сих пор говорилось, Raft может выполнять команду много раз: для примера, если произошёл отказ лидера после применения записи журнала но перед ответом клиенту, клиент посторит команду на новом лидере, т.е. второй раз. Решением для клинетов является назначение каждой команде уникального серийного номера. Затем конечные автоматы отслеживают последний обработанный серийный номер вместе с соответсвующим ответом. Если конченый автомат получает команду с уже обработанным серийным номером, то он немедленно отвечает не выполняя запрос.

Операции чтения могут быть выполнены вообще без записи в журнал. Однако, без дополнительных проверок это может привести к риску вернуть устаревшие данные, поскольку лидер, отвечающий на запрос может быть заменён новым лидером, о котором ничего не известно. Линейные чтения не должны возвращать устаревшие данные и Raft должен выполнить двыполнить два условия, чтобы гаранитровать это без использования журналов. Во-первых, лидер должен должен иметь последнюю информацию о том, какие записи были применены. Свойство полноты лидера гарантирует, что лидер имеет все применённые записи, но на момент начала интервала он может не владеть всей информацией. Чтобы иметь применённые записи, он должен применить их в своём интервале. В Raft данная ситуация обрабатыватеся применением каждым лидером пустой записи в начале интервала. Во-вторых, перед обработкой запроса на чтение лидер должен проверить, не был ли он свергнут (его информация может быть устаревшей, если был выбран новый лидер). В Raft это решается обменом heratbeat сообщениями каждым лидером с большинством кластера перед ответом на запрос чтения.

## 9. Реализация и оценка

Мы реализовали Raft как часть реплицируемого кончного автомата, который хранит конфигурацию для RAMCloud [33] и участвует в отказоустойчивости координатора RAMCloud. Реализация Raft содержит около 2000 строк кода C++, не включая тесты, комментарии или пустые строки. Исходный код находится в свободном доступе [23]. Также имеется около 25 независимых сторонних open-source реализаций [34] в различных стадиях разработки, основанных на данном документе. Также несколько компаний разворачивают системы [34], основанные на Raft.

В данной статье производится оценка Raft по трём критериям: ясность, корректность и производительность.

### 9.1. Ясность

Чтобы измерить ясность или понимаемость Raft по сравнению с Paxos мы провели экспериментальное обучение для студентов курса Advanced Operating System Стендвордского Университета, а также для курса Distributed Computing в U.C. Berkley. Мы записали видео лекции по Raft и по Paxos и провели соответствующие опросы. Лекция по Raft освещала все вопросы, рассмотренные в данной статье за исключением уплотнения журналов; лекция по Paxos освещала материал, достаточный для создания эквивалентного реплицируемого конечного автомата и включала в себя single-decree Paxos, multi-decree Paxos, переконфигурацию, и некоторое количество оптимизаций, необходимых на практике (таких как выбор лидера). В опросах выявлялось базовое понимание алгоритмов и от студентов требовалось обосновать основные случаи. Каждый студент просматривал первое видео и отвечал на первый опрос, затем просматривал второе видео и отвечал на второй опрос. Примерно половина участников первым просматривала видео о Paxos и вторая половина о Raft чтобы учесть индивидуальные различия в понимании и влияние опыта, полученного от просмотра первого видео. Мы сравнили баллы, набранные участинками в каждом опросе, чтобы определить, показали ли участники лучшее понимание Raft.

Мы пытались проводить сравнение между Paxos и Raft настолько честно, насколько это возможно. У Paxos в эксперименте были два преимущества: 15 из 43 участников уже имели опыт работы с Paxos и видео о Paxos было на 14% длиннее, чем видео о Raft. Как показано в таблице 1 мы предприняли шаги для смягчения возможных источников необъективности. Все наши результаты доступны для проверки [28, 31].

В среднем, участники набрали на 4,9 балла больше на тесте по Raft (из возможных 60-ти баллов по Raft набрано 25,7 баллов, для Paxos &mdash; 20,8); на рис. 14 показаны индивидуальные баллы.

...

## Ссылки

[^15]: L AMPORT , L. The part-time parliament. _ACM Transactions on Computer Systems 16_, 2 (May 1998), 133–169.
